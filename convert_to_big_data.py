"""
Used to initially convert decks to
the big data.json file.
Has some usful functions and variables too
"""
import json
from json.decoder import JSONDecodeError
import re
import os
from scraper import convert_word_to_hiragana

big_data_dictionary = {}

BIG_DATA_FILE = "big_data.json"
PRIORITY_ORDER = [
    "5. æ•…äº‹ãƒ»ã“ã¨ã‚ã–ãƒ»æ…£ç”¨å¥ã‚ªãƒ³ãƒ©ã‚¤ãƒ³",
    "7. ä¸‰çœå ‚å›½èªè¾å…¸",
    "2. å®Ÿç”¨æ—¥æœ¬èªè¡¨ç¾è¾å…¸",
    "1. å¤§è¾æ³‰",
    "3. å¤§è¾æ—",
    "6. æ—ºæ–‡ç¤¾å›½èªè¾å…¸ ç¬¬åä¸€ç‰ˆ",
    "4. ä½¿ã„æ–¹ã®åˆ†ã‹ã‚‹ é¡èªä¾‹è§£è¾å…¸",
    "8. Weblio",
]
OPENING_BRACKETS = r"ï¼ˆã€Œ\[ã€ã€”\(ã€ï¼»ã€ˆã€Šã€”ã€˜"
CLOSING_BRACKETS = r"ï¼‰ã€\]ã€‘ã€•\)ã€ï¼½ã€‰ã€‹ã€•ã€™"

KANJI = rf"\u3400-\u4dbf\u4e00-\u9fff\uf900-\ufaff\uff66-\uff9f"
HIRAGANA = rf"ã‚-ã‚”"
KANA = rf"ã‚-ãƒº"
NUMBER_CHARS = r"â‘ -â‘³â¶-â¿ã‰‘-ã‰Ÿâ‘´-â’‡â’ˆ-â’›âŠ-â“â€-â‰ğŸˆ©ğŸˆ”ğŸˆªãŠ€-ãŠ‰ãŠ¤ãŠ¥ãŠ¦ã‹-ã‹¾ï¼‘-ï¼™â“-â“©â’¶-â“ğŸ…-ğŸ…©"
FIRST_NUMBER_CHARS = r"â‘ â¶â‘´â’ˆâŠâ€ğŸˆ©ãŠ€ãŠ¤ã‹ï¼‘â“â’¶ğŸ…"
LAST_NUMBER_CHARS = r"â‘³â¿â‘³â’‡â’›â“â‰ğŸˆªãŠ‰ãŠ¦ã‹¾ï¼™â“©â“ğŸ…©"
NUMBERS_AND_EMOJIS = rf"[{NUMBER_CHARS}]|\dï¸âƒ£"
PREFIX = rf"{NUMBERS_AND_EMOJIS}|^|ã€‚|ãƒ»|<br\/>&nbsp;|\n|[{CLOSING_BRACKETS}{OPENING_BRACKETS}]| |ã€€|è¨˜å·.+?"
SUFFIX = rf"ã€‚|\n|<br\/>&nbsp;"
ARROWS = rf"â‡”â†’â†â˜â‡’â‡â‡¨"
NUMBER_CATEGORIES = {
    "â‘ ": "â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©â‘ªâ‘«â‘¬â‘­â‘®â‘¯â‘°â‘±â‘²â‘³ã‰‘ã‰’ã‰“ã‰”ã‰•ã‰–ã‰—ã‰˜ã‰™ã‰šã‰›ã‰œã‰ã‰ã‰Ÿ",
    "â¶": "â¶â·â¸â¹âºâ»â¼â½â¾â¿",
    "â‘´": "â‘´â‘µâ‘¶â‘·â‘¸â‘¹â‘ºâ‘»â‘¼â‘½â‘¾â‘¿â’€â’â’‚â’ƒâ’„â’…â’†â’‡",
    "â’ˆ": "â’ˆâ’‰â’Šâ’‹â’Œâ’â’â’â’â’‘â’’â’“â’”â’•â’–â’—â’˜â’™â’šâ’›",
    "âŠ": "âŠâ‹âŒâââââ‘â’â“",
    "â€": "â€ââ‚âƒâ„â…â†â‡âˆâ‰",
    "ğŸˆ©": "ğŸˆ©ğŸˆ”ğŸˆª",
    "ãŠ€": "ãŠ€ãŠãŠ‚ãŠƒãŠ„ãŠ…ãŠ†ãŠ‡ãŠˆãŠ‰",
    "ãŠ¤": "ãŠ¤ãŠ¥ãŠ¦]",
    "ã‹": "ã‹ã‹‘ã‹’ã‹“ã‹”ã‹•ã‹–ã‹—ã‹˜ã‹™ã‹šã‹›ã‹œã‹ã‹ã‹Ÿã‹ ã‹¡ã‹¢ã‹£ã‹¤ã‹¥ã‹¦ã‹§ã‹¨ã‹©ã‹ªã‹«ã‹¬ã‹­ã‹®ã‹¯ã‹°ã‹±ã‹²ã‹³ã‹´ã‹µã‹¶ã‹·ã‹¸ã‹¹ã‹ºã‹»ã‹¼ã‹½ã‹¾",
    "ï¼‘": "ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™",
    "â“": "â“â“‘â“’â““â“”â“•â“–â“—â“˜â“™â“šâ“›â“œâ“â“â“Ÿâ“ â“¡â“¢â“£â“¤â“¥â“¦â“§â“¨â“©",
    "â’¶": "â’¶â’·â’¸â’¹â’ºâ’»â’¼â’½â’¾â’¿â“€â“â“‚â“ƒâ“„â“…â“†â“‡â“ˆâ“‰â“Šâ“‹â“Œâ“â“â“",
    "ğŸ…": "ğŸ…ğŸ…‘ğŸ…’ğŸ…“ğŸ…”ğŸ…•ğŸ…–ğŸ…—ğŸ…˜ğŸ…™ğŸ…šğŸ…›ğŸ…œğŸ…ğŸ…ğŸ…ŸğŸ… ğŸ…¡ğŸ…¢ğŸ…£ğŸ…¤ğŸ…¥ğŸ…¦ğŸ…§ğŸ…¨ğŸ…©",
    "KeyCapEmoji": ["1ï¸âƒ£", "2ï¸âƒ£", "3ï¸âƒ£", "4ï¸âƒ£", "5ï¸âƒ£", "6ï¸âƒ£", "7ï¸âƒ£", "8ï¸âƒ£", "9ï¸âƒ£"]  # In array because key is more than 1 character
}
NUMBER_CATEGORIES_REGEX = {
    "â‘ ": r"[â‘ -â‘³ã‰‘-ã‰Ÿ]+",
    "â¶": r"[â¶-â¿]+",
    "â‘´": r"[â‘´-â’‡]+",
    "â’ˆ": r"[â’ˆ-â’›]+",
    "âŠ": r"[âŠ-â“]+",
    "â€": r"[â€-â‰]+",
    "ğŸˆ©": r"[ğŸˆ©ğŸˆ”ğŸˆª]+",
    "ãŠ€": r"[ãŠ€-ãŠ‰]+",
    "ãŠ¤": r"[ãŠ¤-ãŠ¦]+",
    "ã‹": r"[ã‹-ã‹¾]+",
    "ï¼‘": r"[ï¼-ï¼™]+",
    "â“": r"[â“-â“©]+",
    "â’¶": r"[â’¶-â“]+",
    "ğŸ…": r"[ğŸ…-ğŸ…©]+",
    "KeyCapEmoji": r"(?:\d+ï¸âƒ£)+",
}

# Maybe there's a smarter way to do this but lololol
REFERENCE_NUMBER_MAP = {
    # Circled Numbers
    "â‘ ": 1,
    "â‘¡": 2,
    "â‘¢": 3,
    "â‘£": 4,
    "â‘¤": 5,
    "â‘¥": 6,
    "â‘¦": 7,
    "â‘§": 8,
    "â‘¨": 9,
    "â‘©": 10,
    "â‘ª": 11,
    "â‘«": 12,
    "â‘¬": 13,
    "â‘­": 14,
    "â‘®": 15,
    "â‘¯": 16,
    "â‘°": 17,
    "â‘±": 18,
    "â‘²": 19,
    "â‘³": 20,
    # Parenthesized Numbers
    "â‘´": 1,
    "â‘µ": 2,
    "â‘¶": 3,
    "â‘·": 4,
    "â‘¸": 5,
    "â‘¹": 6,
    "â‘º": 7,
    "â‘»": 8,
    "â‘¼": 9,
    "â‘½": 10,
    "â‘¾": 11,
    "â‘¿": 12,
    "â’€": 13,
    "â’": 14,
    "â’‚": 15,
    "â’ƒ": 16,
    "â’„": 17,
    "â’…": 18,
    "â’†": 19,
    "â’‡": 20,
    "1ï¸âƒ£": 1,
    "2ï¸âƒ£": 2,
    "3ï¸âƒ£": 3,
    "4ï¸âƒ£": 4,
    "5ï¸âƒ£": 5,
    "6ï¸âƒ£": 6,
    "7ï¸âƒ£": 7,
    "8ï¸âƒ£": 8,
    "9ï¸âƒ£": 9,
    # Double Circled Numbers
    "â¶": 1,
    "â·": 2,
    "â¸": 3,
    "â¹": 4,
    "âº": 5,
    "â»": 6,
    "â¼": 7,
    "â½": 8,
    "â¾": 9,
    "â¿": 10,
    # Enclosed Alphanumeric Supplement
    "ã‰‘": 21,
    "ã‰’": 22,
    "ã‰“": 23,
    "ã‰”": 24,
    "ã‰•": 25,
    "ã‰–": 26,
    "ã‰—": 27,
    "ã‰˜": 28,
    "ã‰™": 29,
    "ã‰š": 30,
    "ã‰›": 31,
    "ã‰œ": 32,
    "ã‰": 33,
    "ã‰": 34,
    "ã‰Ÿ": 35,
    # Enclosed CJK Ideographic Supplement
    "ãŠ€": 1,
    "ãŠ": 2,
    "ãŠ‚": 3,
    "ãŠƒ": 4,
    "ãŠ„": 5,
    "ãŠ…": 6,
    "ãŠ†": 7,
    "ãŠ‡": 8,
    "ãŠˆ": 9,
    "ãŠ‰": 10,
    # Enclosed CJK Ideographic Units
    "ãŠ¤": "ä¸Š",
    "ãŠ¥": "ä¸­",
    "ãŠ¦": "ä¸‹",
    # Enclosed Katakana
    "ã‹": "ã‚¢",
    "ã‹‘": "ã‚¤",
    "ã‹’": "ã‚¦",
    "ã‹“": "ã‚¨",
    "ã‹”": "ã‚ª",
    "ã‹•": "ã‚«",
    "ã‹–": "ã‚­",
    "ã‹—": "ã‚¯",
    "ã‹˜": "ã‚±",
    "ã‹™": "ã‚³",
    "ã‹š": "ã‚µ",
    "ã‹›": "ã‚·",
    "ã‹œ": "ã‚¹",
    "ã‹": "ã‚»",
    "ã‹": "ã‚½",
    "ã‹Ÿ": "ã‚¿",
    "ã‹ ": "ãƒ",
    "ã‹¡": "ãƒ„",
    "ã‹¢": "ãƒ†",
    "ã‹£": "ãƒˆ",
    "ã‹¤": "ãƒŠ",
    "ã‹¥": "ãƒ‹",
    "ã‹¦": "ãƒŒ",
    "ã‹§": "ãƒ",
    "ã‹¨": "ãƒ",
    "ã‹©": "ãƒ",
    "ã‹ª": "ãƒ’",
    "ã‹«": "ãƒ•",
    "ã‹¬": "ãƒ˜",
    "ã‹­": "ãƒ›",
    "ã‹®": "ãƒ",
    "ã‹¯": "ãƒŸ",
    "ã‹°": "ãƒ ",
    "ã‹±": "ãƒ¡",
    "ã‹²": "ãƒ¢",
    "ã‹³": "ãƒ¤",
    "ã‹´": "ãƒ¦",
    "ã‹µ": "ãƒ¨",
    "ã‹¶": "ãƒ©",
    "ã‹·": "ãƒª",
    "ã‹¸": "ãƒ«",
    "ã‹¹": "ãƒ¬",
    "ã‹º": "ãƒ­",
    "ã‹»": "ãƒ¯",
    "ã‹¼": "ãƒ°",
    "ã‹½": "ãƒ±",
    "ã‹¾": "ãƒ²",
    # Fullwidth Numbers
    "ï¼‘": 1,
    "ï¼’": 2,
    "ï¼“": 3,
    "ï¼”": 4,
    "ï¼•": 5,
    "ï¼–": 6,
    "ï¼—": 7,
    "ï¼˜": 8,
    "ï¼™": 9,
    # 'ï¼‘ï¼‘': 11, 'ï¼‘ï¼’': 12, 'ï¼‘ï¼“': 13, 'ï¼‘ï¼”': 14, 'ï¼‘ï¼•': 15, 'ï¼‘ï¼–': 16, 'ï¼‘ï¼—': 17, 'ï¼‘ï¼˜': 18, 'ï¼‘ï¼™': 19,
    # 'ï¼’ï¼': 20,
    # Enclosed Alphanumeric
    "â“": "a",
    "â“‘": "b",
    "â“’": "c",
    "â““": "d",
    "â“”": "e",
    "â“•": "f",
    "â“–": "g",
    "â“—": "h",
    "â“˜": "i",
    "â“™": "j",
    "â“š": "k",
    "â“›": "l",
    "â“œ": "m",
    "â“": "n",
    "â“": "o",
    "â“Ÿ": "p",
    "â“ ": "q",
    "â“¡": "r",
    "â“¢": "s",
    "â“£": "t",
    "â“¤": "u",
    "â“¥": "v",
    "â“¦": "w",
    "â“§": "x",
    "â“¨": "y",
    "â“©": "z",
    # Enclosed Latin Letters
    "â’¶": "A",
    "â’·": "B",
    "â’¸": "C",
    "â’¹": "D",
    "â’º": "E",
    "â’»": "F",
    "â’¼": "G",
    "â’½": "H",
    "â’¾": "I",
    "â’¿": "J",
    "â“€": "K",
    "â“": "L",
    "â“‚": "M",
    "â“ƒ": "N",
    "â“„": "O",
    "â“…": "P",
    "â“†": "Q",
    "â“‡": "R",
    "â“ˆ": "S",
    "â“‰": "T",
    "â“Š": "U",
    "â“‹": "V",
    "â“Œ": "W",
    "â“": "X",
    "â“": "Y",
    "â“": "Z",
    # Fullwidth Latin Letters (uppercase)
    "ğŸ…": "A",
    "ğŸ…‘": "B",
    "ğŸ…’": "C",
    "ğŸ…“": "D",
    "ğŸ…”": "E",
    "ğŸ…•": "F",
    "ğŸ…–": "G",
    "ğŸ…—": "H",
    "ğŸ…˜": "I",
    "ğŸ…™": "J",
    "ğŸ…š": "K",
    "ğŸ…›": "L",
    "ğŸ…œ": "M",
    "ğŸ…": "N",
    "ğŸ…": "O",
    "ğŸ…Ÿ": "P",
    "ğŸ… ": "Q",
    "ğŸ…¡": "R",
    "ğŸ…¢": "S",
    "ğŸ…£": "T",
    "ğŸ…¤": "U",
    "ğŸ…¥": "V",
    "ğŸ…¦": "W",
    "ğŸ…§": "X",
    "ğŸ…¨": "Y",
    "ğŸ…©": "Z",
}


def convert_reference_numbers(text):
    """Convert reference numbers in text to the format (number)."""

    # Function to replace each match with its mapped numeric value
    def replace_match(match):
        char = match.group(0)
        number = REFERENCE_NUMBER_MAP.get(char)
        return (
            f"ã€š{number}ã€›" if number else char
        )  # Return the number in parentheses or the char itself

    # Substitute each reference character with the desired format
    result = re.sub(
        r"|".join(map(re.escape, REFERENCE_NUMBER_MAP.keys())), replace_match, text
    )
    return result


def dict_to_text(d, level=0):
    """Convert a nested dictionary to a formatted string with indentation based on nesting level."""
    result = ""

    for key, value in d.items():
        if value in ["", ":", "\n"]:
            continue

        # Add a newline, then tabs based on the current level
        prefix = "â””" if level == 0 else "â””" + "â”€" * level
        result += "\n" + prefix + key

        # If the value is a string, add it after the key
        if isinstance(value, str):
            value = re.sub(r"^:", "", value)
            result += " " + value
        # If the value is a nested dictionary, recursively convert it
        elif isinstance(value, dict):
            result += dict_to_text(value, level + 1)


    result = re.sub(rf"(?:<br/>&nbsp;|\n)+", r"\n", result)
    result = re.sub(rf"^(â””â”€*)({NUMBERS_AND_EMOJIS})â””â”€*({NUMBERS_AND_EMOJIS})", 
                r"\1\2 \3", 
                result
              )
    a = result[:]
    result = re.sub(r"^(â””â”€*)(\n|<br/>&nbsp;|$)", 
                     "", 
                     result
                    )
    return result


def find_first_category(text):
    """Identify the first number category that appears in the text."""
    first_category = None
    earliest_index = len(text) + 1  # Beyond bounds
    for category, pattern in NUMBER_CATEGORIES_REGEX.items():
        match_object = re.search(pattern, text)
        if match_object:
            start_index = match_object.span()[0]
            if start_index < earliest_index:
                earliest_index = start_index
                first_category = category

    return first_category


def segment_by_category(text, category, first_category, level):
    """Segments text by the first number characters of a specified category.
       If a key has a lower value than the previous or a jump of 2 or more,
       it includes that key and the rest of the segment in the key's segment."""
    
    # Get the pattern for the category and initialize tracking variables
    pattern = NUMBER_CATEGORIES_REGEX[category]
    segments_dict = {}
    segments = re.split(f"({pattern[:-1]})", text)
    previous = 0  # Keep track of the last processed key's value
    previous_key = None
    i = 0
    while i < len(segments) - 1:
        if re.match(pattern, segments[i]):
            key = segments[i]
            current_number = NUMBER_CATEGORIES[category].index(key) + 1
            # Check if the current key is valid based on previous key's value
            # print(f"{current_number=} {previous=} {key=} {previous_key=}")
            is_referencing_other_level = level > 0 and first_category == category
            if is_referencing_other_level or (current_number <= previous or current_number > previous + 1):
                # If the current key is lower or jumps 2 or more, we're talking about a different key in reference
                segments_dict[previous_key] += key + ''.join(segments[i + 1]).strip()
            else:
                # Otherwise, add the segment normally
                segments_dict[key] = segments[i + 1].strip()
                previous = current_number  # Update highest
                previous_key = key

            i += 2  # Move to the next potential key-value pair
        else:
            i += 1  # Move to the next segment if not a key pattern match
    # print("\n\n")
    # if "0" in segments_dict:
        # print(segments_dict)
    return segments_dict

def recursive_nesting_by_category(text, first_category=None, next_category=None, level=0):
    """Recursively separates the text into nested dictionaries by number character categories."""
    next_category = find_first_category(text)
    if not next_category:
        return text  # Base case: no number characters left
    if not first_category:
        first_category = next_category

    try:
        segments_dict = segment_by_category(text, first_category, next_category, level=level)
    except KeyError:
        return text  # Text, no longer has any segments

    for key, sub_text in segments_dict.items():
        segments_dict[key] = recursive_nesting_by_category(
            sub_text, 
            first_category=first_category, 
            next_category=next_category, 
            level=level+1
        )

    return segments_dict


def get_entry(ref_path, text):
    if not ref_path:
        return text

    entry_dict = recursive_nesting_by_category(text)

    current = entry_dict.copy()

    for step in ref_path:
        find_correct = [
            k for k in current.keys() if str(REFERENCE_NUMBER_MAP[k]) == step
        ]
        if find_correct:
            current = current[find_correct[0]]
        else:
            return current

    if isinstance(current, str):
        return current  # Final destination
    elif isinstance(current, dict):
        # Current is still a nested dictionary
        return dict_to_text(current)


def convert_to_path(reference_numbers):
    path = []
    counter = 0
    for i, x in enumerate(reference_numbers):
        if counter > 0:
            counter -= 1
            continue

        if x <= "9" and counter == 0:
            path.append(f"{x}{reference_numbers[i + 1]}{reference_numbers[i + 2]}")
            counter = 2
        else:
            path.append(x)

    return path


def add_dictionary_to_big_data(dictionary_path, big_data):
    """
    Adds words and their definitions from dictionary files to the global `big_data` dictionary.

    Args:
    - dictionary_path (str): Path to the dictionary folder.
    - big_data (dict): The shared dictionary.
    """
    print(f"Adding from {dictionary_path}")
    term_bank_files = sorted(
        [
            f
            for f in os.listdir(dictionary_path)
            if re.match(r"term_bank_\d+\.json$", f)
        ],
        key=lambda x: int(re.search(r"\d+", x).group()),
    )

    for file in term_bank_files:
        # data = None
        # with open(f"6. æ—ºæ–‡ç¤¾å›½èªè¾å…¸ ç¬¬åä¸€ç‰ˆ/{file}", "r", encoding="utf-8") as f:
        #     data = json.load(f)

        # with open(f"6. æ—ºæ–‡ç¤¾å›½èªè¾å…¸ ç¬¬åä¸€ç‰ˆ/{file}", "w", encoding="utf-8") as f:
        #     json.dump(data, f, indent=2, ensure_ascii=False)

        process_term_bank_file(file, dictionary_path, big_data)


def process_term_bank_file(file, dictionary_path, big_data):
    """Processes a single term bank file."""
    print(f"Processing {file} in {dictionary_path}")

    if dictionary_path not in big_data:
        big_data[dictionary_path] = {}

    file_path = os.path.join(dictionary_path, file)
    words_to_remove = []

    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
            # if dictionary_path.endswith("å¤§è¾æ—"):
            #     data = data[0]
            for i, entry in enumerate(data):
                word, reading, entry_type, definitions_in_data = entry[0], entry[1], entry[2], entry[5]
                # "å¤§è¾æ—" dictionary thing. 
                if entry_type not in ["å­", "å¥"]:
                
                    # Words can have different readings, and different definitions.
                    # æœ€ä¸­ï¼ˆã•ã„ã¡ã‚…ã†ãƒ»ã•ãªã‹ï¼‰etc.
                    # If a word has multiple definitions for a single reading, run over all of them.
                    # We are currently handling a single entry.

                    if not reading:
                        reading = convert_word_to_hiragana(word)
                    else:
                        reading = convert_word_to_hiragana(reading)

                    definition_list = []

                    for definition in definitions_in_data:
                        definition_text = get_text_only_from_dictionary(
                            word, reading, definition, dictionary_path
                        )
                        if definition_text:
                            definition_list.append(definition_text)
                        # else:
                            # print(json.dumps(definition, ensure_ascii=False, indent=2))
                            # print("\n")
                    # Create new entry/extend existing one
                else:
                    definition_list = []

                # if not definition_list and entry_type not in ["å­", "å¥"]:
                #     print(word)
                #     print(json.dumps(entry, ensure_ascii=False, indent=2))

                if not definition_list:  # No definitions for entry?
                    words_to_remove.append(word)
                else:
                    edit_big_data(
                        big_data, dictionary_path, word, reading, definition_list
                    )

    except JSONDecodeError as e:
        print(f"Error decoding JSON in file {file_path}: {e}")
        return

    # Filter out words to remove
    data_after_edits = [item for item in data if item[0] not in words_to_remove]

    print(f"Size after removals: {len(data_after_edits)}")

    # with open(file_path, "w", encoding="utf-8") as f:
    #     json.dump(data_after_edits, f, ensure_ascii=False, indent=2)

def replace_furigana_references(text):
    text = text.replace("ï¼ˆ", " (").replace("ï¼‰", ") ")
    a_prefix = rf"({PREFIX})?"
    words_and_furigana = rf"((?:([{KANJI}]+)(?: \([{HIRAGANA}]+)\) ?)+)([{HIRAGANA}]+)?"
    a_suffix = rf"((?:{NUMBERS_AND_EMOJIS})+)?"
    ref_with_furigana = re.compile(
        rf"{a_prefix}â‡’{words_and_furigana}{a_suffix}",
        flags=re.UNICODE,
    )
    match_object = ref_with_furigana.finditer(text)
    match_object_2 = ref_with_furigana.finditer(text)
    # print([g for g in match_object_2])
    links = []

    if match_object:
        for match in match_object:

            # Since we can't just "guess" the kanji's readings,
            # I'm only taking reading into account when it 
            # describes the entire word.

            reading_match = re.search(rf"( \(([{HIRAGANA}]+)\) ?)(?:(?:[{NUMBER_CHARS}]|(\dï¸âƒ£))+|\n|$)", match.group())
            has_kanji = re.search(rf"[{KANJI}]", match.group())
            furigana = None

            if reading_match and has_kanji:
                matched = reading_match.group(1)
                furigana = "".join(["".join([y if y else "" for y in x]) for x in matched])

            no_furigana_and_ref = re.findall(
                rf"[a-zA-Z]|[{KANJI}]|(?:{NUMBERS_AND_EMOJIS})+$|[^(â‡’][{HIRAGANA}]+[^)]?$",
                match.group(),
                flags=re.U,
            )

            number = match.groups()[3] if match.groups()[3] else ""

            if no_furigana_and_ref:
                no_furigana_and_ref = "".join(
                    [x.replace(" ", "") for x in no_furigana_and_ref]
                )

            # No furigana that describes the entire word 
            # â†“
            # Replace with the no-furigana version

            # Has furigana that describes the entire word
            # â†“
            # Don't replace

            original = f"{match.group(2)}{match.group(4) if match.group(4) else ''}"
            if not furigana:
                text = text.replace(original, no_furigana_and_ref)

    return text

def normalize_references(text: str, dictionary_path: str) -> str:
    text = re.sub(rf" ?[{ARROWS}]", "â‡’", text)
    text = text.replace("\\n", "\n")
    text = text.replace("\\n", "\n")
    flag = False
    text_original = text[:]

    if dictionary_path.endswith("å¤§è¾æ³‰"):
        # ï¼»åï¼½(ã‚¹ãƒ«)ã€Œã‚¢ãƒ«ãƒã‚¤ãƒˆã€ã®ç•¥ã€‚ã€Œå¤ä¼‘ã¿ã«ãƒã‚¤ãƒˆã™ã‚‹ã€
        has_ryaku = re.compile(
            rf"({PREFIX})ã€Œ([^{OPENING_BRACKETS}]+?)ã€ã®ç•¥({SUFFIX})"
        )
        if has_ryaku.search(text):
            adding_text = has_ryaku.sub(r"â‡’\2", has_ryaku.search(text).group())
            if adding_text not in text:
                text += "\n" + adding_text

        # Handle this?
        # ï¼»é€£èªï¼½â‡’ç½® (ãŠ) ãâ‘«
        # â‡’äººè¿” (ã²ã¨ãŒãˆ) ã—â‘¡

        """
        0-11  â‡’ç•°åŒ– (ã„ã‹) â‘¡
        0-1  
        2-10  ç•°åŒ– (ã„ã‹) 
        2-4   ç•°åŒ–
        10-11 â‘¡
        """
        
                # flag = True
        text = replace_furigana_references(text)
        text = re.sub(r"â‡’{2,}", "â‡’", text)
        text = text.replace("\\n", "\n")
        # convert ã«åŒã˜ format to â‡’ format for linking purposes later.
        # Either in the beginning, between lines, or between periods.
        #                             Prefix   Word        Suffix
        definition_text = re.sub(
            rf"({PREFIX})ã€Œ(.+?)ã€ã«åŒã˜({SUFFIX})", r"\1â‡’\2\3", text
        )

        # ã€‚ã€Œè¨€è‘‰â‘ ã€ã«åŒã˜ã€‚
        # ã€‚â‡’è¨€è‘‰â‘ 
        # ã€‚â‡’è¨€è‘‰(1) (Later)

        # ã€‚â‡’å†…åŒ å¯® (ãŸãã¿ã‚Šã‚‡ã†) â‘ 
        # ã€‚â‡’IOAï¼ˆIndependent Olympic Athletesï¼‰
        # â‡’ã‚³ãƒãƒ¼ã‚·ãƒ£ãƒ«â‘ 
        # ...ãƒ©ãƒãƒ¼ã€‚â†’å¼¾æ€§ã‚´ãƒ \nâ‘¡ æ¤ç‰©ã‹ã‚‰...
        # â‡’é‰±å·¥æ¥­ç”Ÿç”£æŒ‡æ•°â‘ 
        #                         To not include the number chars we want later. However, DO include digits, but not if follwoed by emoji data.
        #                                                         Prefix                         Word                                   SOMETHING IN BRACKETS                        Reference Num or whatever

        pattern_text = rf"({PREFIX})?â‡’([^\n]+?)(?:ï¼ˆ.+?ï¼‰)?((?:{NUMBERS_AND_EMOJIS})*?)({SUFFIX}|$|ãƒ»| |ã€€)"
        pattern = re.compile(pattern_text)

        results = pattern.finditer(text)

        if results:
            # if len([x for x in results]) > 1:

            for result in results:
                _prefix, word, reference_number, suffix = result.groups()
                reference_number = reference_number if reference_number else ""

                # Reference number
                reference_numbers = convert_to_path(reference_number)
                try:
                    references = "".join(
                        [convert_reference_numbers(x) for x in reference_numbers]
                    )
                except KeyError:
                    print("[ERROR]\t", text, reference_numbers)


                text = pattern.sub(f"{_prefix}â‡’{word}{reference_number}{suffix}", text)

    if dictionary_path.endswith("ä¸‰çœå ‚å›½èªè¾å…¸"):

        # â‡’è„‡â‘¦ãƒ»æŒ™ã’å¥â‘¡ã€‚
        # â†“
        # â‡’è„‡â‘¦ã€€â‡’æŒ™ã’å¥â‘¡ã€‚
        reference_pattern = rf"([^\nãƒ»{NUMBER_CHARS}\dï¸âƒ£ï¼ˆ ]+)(?:ï¼ˆ.+?ï¼‰)?(?:([{NUMBER_CHARS}\dï¸âƒ£])*)"

        pattern_mulitple = re.compile(
            rf"â‡’{reference_pattern}((?:ãƒ»{reference_pattern}?(?=ãƒ»|$|\n| |ã€€|ã€‚))+)"
        )

        results_multiple = pattern_mulitple.finditer(text)

        text_original = text[:]
        if results_multiple:
            for result in results_multiple:
                reference_2_and_onwards = result.group(3).split("ãƒ»")[1:]

                for reference in reference_2_and_onwards:
                    reference_word, reference_number = re.search(reference_pattern, reference).groups()
                    reference_number = reference_number if reference_number else ""
                    # Doing this later anyway
                    # reference_numbers = convert_to_path(reference_number)
                    # try:
                    #     references = "".join(
                    #         [convert_reference_numbers(x) for x in reference_numbers]
                    #     )
                    # except KeyError:
                    #     print("[ERROR]\t", text, reference_numbers)

                    text = text.replace(f"ãƒ»{reference}", f" â‡’{reference}")


        text = replace_furigana_references(text)

        # â‘ æœã€‚åˆå‰ã€‚            â˜“
        # â‘¡ã€˜æœã€™â†ãƒ¢ãƒ¼ãƒ‹ãƒ³ã‚°ã‚³ãƒ¼ãƒˆã€‚ â—¯
        # â‘¢â†ãƒ¢ãƒ¼ãƒ‹ãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹ã€‚ã€€ã€€ â—¯
        # (!) Remeber,   All arrows are now "â‡’"

        pattern = re.compile(
            rf"({NUMBERS_AND_EMOJIS}|^|ã€‚|<br\/>&nbsp;|\n|[{CLOSING_BRACKETS}{OPENING_BRACKETS}]| |ã€€|è¨˜å·.+?)â‡’([{NUMBER_CHARS}]*)([^\d{OPENING_BRACKETS}]+?)([{NUMBER_CHARS}]|\dï¸)?(ãƒ»|$|ã€‚|<br\/>&nbsp;|\n)"
        )
        results = pattern.finditer(text)
        for result in results:
            _prefix, _, word, reference_number, suffix = result.groups()
            reference_number = reference_number if reference_number else ""

            text = pattern.sub(f"{_prefix}â‡’{word}{reference_number}{suffix}", text)

    if dictionary_path.endswith("å¤§è¾æ—"):

        # ï¼»åï¼½(ã‚¹ãƒ«)ã€Œã‚¢ãƒ«ãƒã‚¤ãƒˆã€ã®ç•¥ã€‚ã€Œå¤ä¼‘ã¿ã«ãƒã‚¤ãƒˆã™ã‚‹ã€
        has_ryaku = re.compile(
            rf"({PREFIX})ã€Œ([^{OPENING_BRACKETS}]+?)ã€ã®ç•¥({SUFFIX})"
        )
        if has_ryaku.search(text):
            adding_text = has_ryaku.sub(r"â‡’\2", has_ryaku.search(text).group())
            if adding_text not in text:
                text += "\n" + adding_text


        reference_pattern = rf"([^\nãƒ»{NUMBER_CHARS}\dï¸âƒ£ï¼ˆ ]+)(?:ï¼ˆ.+?ï¼‰)?(?:([{NUMBER_CHARS}\dï¸âƒ£])*)"

        pattern_mulitple = re.compile(
            rf"â‡’{reference_pattern}((?:ãƒ»{reference_pattern}?(?=ãƒ»|$|\n| |ã€€|ã€‚))+)"
        )

        results_multiple = pattern_mulitple.finditer(text)

        text_original = text[:]
        if results_multiple:
            for result in results_multiple:
                reference_2_and_onwards = result.group(3).split("ãƒ»")[1:]

                for reference in reference_2_and_onwards:


                    reference_word, reference_number = re.search(reference_pattern, reference).groups()
                    reference_number = reference_number if reference_number else ""

                    text = text.replace(f"ãƒ»{reference}", f" â‡’{reference}")

        text = replace_furigana_references(text)
    
    text_original2 = text[:]

    if dictionary_path.endswith("ä½¿ã„æ–¹ã®åˆ†ã‹ã‚‹ é¡èªä¾‹è§£è¾å…¸"):
        ...
    
    if dictionary_path.endswith("æ—ºæ–‡ç¤¾å›½èªè¾å…¸ ç¬¬åä¸€ç‰ˆ"):

        # ã€‚â‡’å¤äºº(1)ï¼šå¤äºº(2)
        # â†“
        # â‡’å¤äºº(1) â‡’å¤äºº(2)ã€‚
        reference_pattern = rf"([^\nãƒ»{NUMBER_CHARS}\dï¸âƒ£ï¼ˆ ]+)(?:ï¼ˆ.+?ï¼‰)?(?:([{NUMBER_CHARS}\dï¸âƒ£])*)"

        pattern_mulitple = re.compile(
            rf"â‡’{reference_pattern}((?:ï¼š{reference_pattern}?(?=ãƒ»|$|\n| |ã€€|ã€‚))+)"
        )

        results_multiple = pattern_mulitple.finditer(text)

        text_original = text[:]
        if results_multiple:
            for result in results_multiple:
                reference_2_and_onwards = result.group(3).split("ï¼š")[1:]

                for reference in reference_2_and_onwards:
                    reference_word, reference_number = re.search(reference_pattern, reference).groups()
                    reference_number = reference_number if reference_number else ""

                    text = text.replace(f"ï¼š{reference}", f" â‡’{reference}")


        # â‡’ã‘ã‚“ï¼ˆçŒ®ï¼‰  -  Hiragana (kanji) 
        hiragana_kanji_references = re.search(rf"â‡’([{HIRAGANA}]+?)ï¼ˆ([{KANJI}]+?)ï¼‰", text)
        if hiragana_kanji_references:
            the_match = hiragana_kanji_references.group()
            the_hiragana = hiragana_kanji_references.group(1)
            the_hiragana = hiragana_kanji_references.group(2)
            text = text.replace(the_match, f"â‡’{KANJI} ({HIRAGANA})")


        # â‡’è¨€èªï¼ˆã’ã‚“ã”ï¼‰- Gengo (Furigana)
        # Change full-width brackets to half-width for later function
        text = re.sub(rf"ï¼ˆ([{HIRAGANA}]+?)ï¼‰", rf" (\1) ", text)
        text = text = replace_furigana_references(text)

        if text.endswith("\nâ‡’ã€Œä½¿ã„åˆ†ã‘ã€"):
            text = text[:-len("\nâ‡’ã€Œä½¿ã„åˆ†ã‘ã€")]



    text = re.sub(rf"ãƒ»(?:[{NUMBER_CHARS}]|\dï¸âƒ£)", "", text)

    # Search for reference pattern in the definition
    reference_matches = re.finditer(
        rf"â‡’([^(]+?)( \([ã‚-ã‚”]+\) )?((?:{NUMBERS_AND_EMOJIS})*)(?:ã€‚|$|\n|<br\/>&nbsp;| |ã€€)", text
    )
    text_original3 = text[:]  # For printing purposes if I need debugging
    # {prefix}{tag}â‡’{word}{references}{suffix}
    # already_linked = []
    # If there's a reference in the definition
    if reference_matches:
        for reference_match in reference_matches:
            last_char = reference_match.group()[-1]
            suffix = last_char if last_char in ["ã€‚", "\n", "ã€€", " ", ";"] else ""
            if suffix == ";" and reference_match.group().endswith("<br/>&nbsp;"):
                suffix = "<br/>&nbsp;"

            referenced_word, furigana, reference_number_path = reference_match.groups()
            furigana = furigana if furigana else ""

            reference_number_path = reference_number_path if reference_number_path else ""
            reference_numbers = convert_to_path(reference_number_path)

            try:
                reference_numbers = "".join(
                    [convert_reference_numbers(x) for x in reference_numbers]
                )
            except KeyError:
                print("[ERROR]\t", text, reference_numbers)

            text = text.replace(reference_match.group(), f" â‡’{referenced_word}{furigana}{reference_numbers}")
            text += suffix

    return text


def clean_definition(
    word: str, reading: str, definition_text: str, dictionary_path: str
) -> str:
    """
    Cleans and formats the definition text based on the specific dictionary.

    Args:
    - definition_text (str): The raw definition text to clean.
    - dictionary_path (str): The name or identifier of the dictionary.

    Returns:
    - str: The cleaned and formatted definition text.
    """
    # Remove the first line for specific dictionaries

    my_word = word == "ã‚¢ã‚¦ã‚¿ãƒ¼"

    if my_word:
        print(f"{word}ã€{reading}ã€‘")
        print(f"{definition_text}")
        print()

    if word.endswith("ã®è§£èª¬"):
        return None
    # Normalize \n's
    definition_text = definition_text.replace("\\n", "\n")
    # Weird character
    definition_text = definition_text.replace("â€‰", " ")

    # Unecessary parts
    definition_text = re.sub(
        r"(?:\[è£œèª¬\]|ï¼»è£œèª¬ï¼½|ï¼»ç”¨æ³•ï¼½|\[ç”¨æ³•\]|\[å¯èƒ½\]|ï¼»å¯èƒ½ï¼½)(?:.|\n)+",
        "",
        definition_text,
    ).strip()

    # I don't even know why this appears at times
    definition_text = definition_text.replace("_x000D_", "")

    # I don't even know why this appears at times
    definition_text = definition_text.replace("_x000D_", "")

    if not word and not reading:
        return None

    # Normalize spaces after numbers:
    definition_text = re.sub(rf"([{NUMBER_CHARS}])[ ]+", r"\1", definition_text)
    

    definition_text = normalize_references(definition_text, dictionary_path)

    # if "â‡’" in definition_text:
    #     print(f"{word}ã€{reading}ã€‘")
    #     print(definition_text)
    #     print()
    # Using endswith because I don't care about their order in the priority (or what order you chose to give them
    # in the folder name). Just matters that it ends with the dictionary name.

    if dictionary_path.endswith("å¤§è¾æ³‰"):
        splitted = definition_text.split("\n")
        if len(splitted) > 1:
            definition_text = "<br/>&nbsp;".join(splitted[1:])  # Remove first line
        # Since we're adding all entries as seperate items in a list, we don't need to clean stuff like this;
        # It can easily be filtered out later.
        # No need to destroy data.

        if "[å¯èƒ½]" in definition_text:
            definition_text = definition_text.split("[å¯èƒ½]")[1]

        # Remove remains of example sentences
        # â‘£: ç´å¾—ã™ã‚‹ã€‚åˆç‚¹ãŒã„ãã€‚ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ» (after parsing)
        definition_text = re.sub(r"ãƒ»{2,}", "", definition_text)

        # ï¼»å‹•ã‚¶ä¸Šä¸€ï¼½ã€Œã¾ã‚“ï¼ˆæ…¢ï¼‰ãšã‚‹ã€ï¼ˆã‚µå¤‰ï¼‰ã®ä¸Šä¸€æ®µåŒ–ã€‚
        # ï¼»å‹•ã‚¶ä¸Šä¸€ï¼½ã€Œã¿ãã‚“ãšã‚‹ã€ï¼ˆã‚µå¤‰ï¼‰ã®ä¸Šä¸€æ®µåŒ–ã€‚ã€Œè©±é¡Œã®å±•è¦§ä¼šã‚’â€•ãƒ»ã˜ã‚‹ã€
        # ï¼»å‹•ã‚¶ä¸Šä¸€ï¼½ã€Œã¦ã‚“ï¼ˆè»¢ï¼‰ãšã‚‹ã€ï¼ˆã‚µå¤‰ï¼‰ã®ä¸Šä¸€æ®µåŒ–ã€‚ã€Œæ”»å‹¢ã«â€•ãƒ»ã˜ã‚‹ã€

        # First fix ã€Œã¦ã‚“ï¼ˆè»¢ï¼‰ãšã‚‹ã€ â†’ "ã€Œè»¢ãšã‚‹ã€"
        definition_text = re.sub(
            r"ã€Œ[ã‚-ã‚“]+ï¼ˆ(.+?)ï¼‰(.+?)ã€", r"ã€Œ\1\2ã€", definition_text
        )

        # Then fix ï¼»å‹•ã‚¶ä¸Šä¸€ï¼½ã€Œè»¢ãšã‚‹ã€ï¼ˆã‚µå¤‰ï¼‰ã®ä¸Šä¸€æ®µåŒ–ã€‚ã€Œæ”»å‹¢ã«â€•ãƒ»ã˜ã‚‹ã€ â†’  "â‡’è»¢ãšã‚‹"
        definition_text = re.sub(
            rf"(?:ï¼».+?ï¼½)ã€Œ(.+?)ã€ã®(?:..?æ®µåŒ–|..èª)({SUFFIX})",
            r"â‡’\1\2",
            definition_text,
        )

        # Remove
        # ï¼»é€£èªï¼½ã€Šå½¢å®¹è©ã€ãŠã‚ˆã³å½¢å®¹è©å‹æ´»ç”¨èªã®é€£ä½“å½¢æ´»ç”¨èªå°¾ã€Œã‹ã‚‹ã€ã«æ¨é‡ã®åŠ©å‹•è©ã€Œã‚ã‚Šã€ã®ä»˜ã„ãŸã€Œã‹ã‚‹ã‚ã‚Šã€ã®éŸ³å¤‰åŒ–ã€‹
        # ï¼»é€£èªï¼½ã€Šé€£èªã€Œã‹ã‚“ã‚ã‚Šã€ã®æ’¥éŸ³ã®ç„¡è¡¨è¨˜ã€‹
        definition_text = re.sub(r"ï¼».+?ï¼½ã€Š.+?ã€‹", r"", definition_text)

        # Remove
        # ã€Œæ†ã’ã«ãŠã—ç«‹ã¡ãŸã‚‹ã“ã¨ãªã©ã¯ã‚ã‚‹ã¾ã˜â€•â—¦ã‚ã‚Šã¨æ€ã™ã‚‚ã®ã‹ã‚‰ã€ã€ˆæºãƒ»è‹¥èœä¸Šã€‰$
        # ã€Œã†ãã¶ã‹ã›çµ¦ãµã“ã¨ã€ã—ã’â€•â—¦ã‚ã‚Šã—ã‹ã°ã€ã€ˆã‹ã’ã‚ãµãƒ»ä¸‹ã€‰$
        definition_text = re.sub(r"^ã€‰.+?ã€ˆã€.+?ã€Œ", r"", definition_text[::-1])[::-1]

        # ï¼»å‹•ãƒ©ä¸‹ä¸€ï¼½ï¼»æ–‡ï¼½ã‹ãã¿ã ãƒ»ã‚‹ï¼»ãƒ©ä¸‹äºŒï¼½
        # ï¼»å‹•ãƒ©äº”ï¼ˆå››ï¼‰ï¼½
        # ï¼»å‹•ã‚µä¸‹ä¸€ï¼½ï¼»æ–‡ï¼½ã‹ãã‚ˆãƒ»ã™ï¼»ã‚µä¸‹äºŒï¼½
        # ï¼»åï¼½(ã‚¹ãƒ«)
        definition_text = re.sub(
            rf"(?:ï¼».+?ï¼½)+(?:[{HIRAGANA}ãƒ»]+ï¼».+?ï¼½)?(?:\(ã‚¹ãƒ«\))?",
            r"",
            definition_text,
        )
    
    if dictionary_path.endswith("æ—ºæ–‡ç¤¾å›½èªè¾å…¸ ç¬¬åä¸€ç‰ˆ"):
        # Remove first line
        # ã‚ã„â€ã—ã‚‡ã†ã€å“€å‚·ã€‘â€•â€•ã‚·ãƒ¤ã‚¦\n
        splitted = definition_text.split("\n")
        if len(splitted) > 1:
            definition_text = "<br/>&nbsp;".join(splitted[1:])  # Remove first line

        # Remove the first line in items like this.
        # ã‚ã„ã€æŒ¨ã€‘\nã‚¢ã‚¤ãŠ¥\nãŠã™\nç­†é †ï¼š\n
        # \n\nï¼ˆå­—ç¾©ï¼‰\nâ‘  ãŠã™ã€‚æŠ¼ã—ã®ã‘ã‚‹ã€‚ã€ŒæŒ¨æ‹¶ï¼ˆã‚ã„ã•ã¤ï¼‰ï¼ˆï¼åŸç¾©ã¯æŠ¼ã—ã®ã‘ã¦é€²ã‚€æ„ã€‚å›½ ...

        if "ç­†é †ï¼š" in definition_text:
            definition_text = definition_text.split("ç­†é †ï¼š")[1]
        definition_text = re.sub(r"å›³ç‰ˆï¼š\n?", "", definition_text) 
        definition_text = definition_text.strip("<br/>&nbsp;")

        # Remove
        # ï¼ˆåãƒ»ä»–ã‚¹ãƒ«ï¼‰\n.
        # ï¼ˆå½¢ï¼‰ã€Šã‚«ãƒ­ãƒ»ã‚«ãƒ„ï¼ˆã‚¯ï¼‰ãƒ»ã‚¤ãƒ»ã‚¤ãƒ»ã‚±ãƒ¬ãƒ»â—‹ã€‹\n
        # But keep (â€¦ã®ç•¥) ?

        definition_text = re.sub(r"ï¼ˆ.+?(?!ã®ç•¥)ï¼‰(ã€Š.+?ã€‹)?\n", "", definition_text)

        # Remove
        # ã€”å¯èƒ½ã€•ã‚ãŒãƒ»ã‚Œã‚‹ï¼ˆä¸‹ä¸€ï¼‰<br/>&nbsp;
        # ã€”ä»–ã€•ã‚ãƒ»ã’ã‚‹ï¼ˆä¸‹ä¸€ ï¼‰
        definition_text = re.sub(
            r"ã€”.+?ã€•?[{HIRAGANA}ãƒ»]+ï¼ˆ.+?ï¼‰({SUFFIX})", r"\1", definition_text
        )

        # Remove everything after ã€˜ä½¿ã„åˆ†ã‘ã€™
        if "ã€˜ä½¿ã„åˆ†ã‘ã€™" in definition_text:
            definition_text = definition_text.split("ã€˜ä½¿ã„åˆ†ã‘ã€™")[0]

        # Remove everything after ã€˜ã¡ãŒã„ã€™
        if "ã€˜ã¡ãŒã„ã€™" in definition_text:
            definition_text = definition_text.split("ã€˜ã¡ãŒã„ã€™")[0]

    if dictionary_path.endswith("ä½¿ã„æ–¹ã®åˆ†ã‹ã‚‹ é¡èªä¾‹è§£è¾å…¸"):
        ...
        # This is already handled in the scraping function.
        # definition_text = definition_text.split(r'ğŸ“šä½¿ã„æ–¹')[0]
        # definition_text = definition_text.split(r'ğŸ”„ä½¿ã„åˆ†ã‘')[0]
        # definition_text = definition_text.split(r'ğŸ”—é–¢é€£èª')[0]

    if dictionary_path.endswith("ä¸‰çœå ‚å›½èªè¾å…¸"):
        ...
        # This is already handled in the scraping function
        # definition_text = re.sub(r"^.+?ï½ <br/>&nbsp;|ã€Œ.+ã€(?:<br/>&nbsp;)?", "", definition_text)

    if dictionary_path.endswith("äº‹æ•…ãƒ»ã“ã¨ã‚ã–ãƒ»æ…£ç”¨å¥ã‚ªãƒ³ãƒ©ã‚¤ãƒ³"):
        ...
        # This is already handled in the scraping function

        # Remove spans like this
        # ã—ã‚Šã¦ã—ã‚‰ã–ã‚Œã€çŸ¥ã‚Šã¦çŸ¥ã‚‰ã–ã‚Œã€‘
        # ã€å¤±æ•—ã¯æˆåŠŸã®ã‚‚ã¨ã€‘

    if dictionary_path.endswith("å¤§è¾æ—"):
        no_period_quote = re.search(r"[^ã€‚ã€]$", definition_text)
        final_word_reference = re.search(
            rf"â‡’[{KANJI}{KANA}a-zA-Zãƒ»]+$", definition_text
        )
        if no_period_quote and not final_word_reference:
            return None
        definition_text = definition_text.split("è£œèª¬æ¬„")[0]

        # This is already handled in the scraping function

    if dictionary_path.endswith("å®Ÿç”¨æ—¥æœ¬èªè¡¨ç¾è¾å…¸"):
        ...
        # This is already handled in the scraping function

    if dictionary_path.endswith("Weblio"):
        # ï¼»å‹•ã‚«ä¸‹ä¸€ï¼½ï¼»æ–‡ï¼½ãªã¤ãƒ»ãï¼»ã‚«ä¸‹äºŒï¼½ã€Šã€Œãªã¥ã‘ã‚‹ã€ã¨ã‚‚ã€‹
        # ï¼»å‹•ã‚¢ä¸‹ä¸€ï¼½ï¼»æ–‡ï¼½ã‹ã¾ãƒ»ãµï¼»ãƒä¸‹äºŒï¼½
        # ï¼»å‹•ã‚«äº”ï¼ˆå››ï¼‰ï¼½
        definition_text = re.sub(
            rf"({PREFIX})(?:ï¼».+?ï¼½)+(?:[{HIRAGANA}ãƒ»]+ï¼».+?ï¼½)?(?:ã€Š.+?ã€‹)?({SUFFIX})",
            r"\1\2",
            definition_text,
        )

    # # Add line breaks before entry numbers
    # definition_text = re.sub(rf"({NUMBERS_AND_EMOJIS})", r"<br/>&nbsp;\1", definition_text)
    # Clean up leading or trailing unwanted characters

    if definition_text:
        definition_text = definition_text.strip("\n").strip("<br/>&nbsp;")
        # once

    # if "â‡’" in definition_text:
    #     definition_text = re.sub(rf"({PREFIX})â‡’([{NUMBER_CHARS}]*)(.+)($|ã€‚|<br/>&nbsp;|\n)", r"\1\2\3\4", definition_text)

    # Normalize numbers back
    definition_text = re.sub(rf"([{NUMBER_CHARS}][^ ]) ", r"\1 ", definition_text)
    # if "éŠé‡Œã§å®¢ã®ç›¸æ‰‹ã¨ãªã‚‹éŠå¥³" in definition_text:
        # print(4, definition_text)
    # Normalize line breaks
    definition_text = definition_text\
                      .replace("\n", "<br/>&nbsp;")\
                      .replace("\\n", "<br/>&nbsp;")
 
    # Contract multiple linebreaks into a single linebreak
    # For some fucking reason {2,} doesn't work so here we are.
    definition_text = re.sub(r"(<br/>&nbsp;|\n|\\n)+", r"<br/>&nbsp;", definition_text)

    # if "éŠé‡Œã§å®¢ã®ç›¸æ‰‹ã¨ãªã‚‹éŠå¥³" in definition_text:
        # print(4.5, definition_text) 

    # Temp
    definition_text = definition_text.replace("<br/>&nbsp;", "\n")

    # if "éŠé‡Œã§å®¢ã®ç›¸æ‰‹ã¨ãªã‚‹éŠå¥³" in definition_text:
    
        # print(5, definition_text)    

    definition_dict = recursive_nesting_by_category(definition_text)
    if isinstance(definition_dict, dict):
        definition_text = dict_to_text(definition_dict)
    else:
        definition_text = definition_dict

    definition_text = re.sub(r"ã€‚ã€‚+", "", definition_text)

    definition_text = definition_text.strip("\n").strip()


    if "[å¯èƒ½]" in definition_text:
        definition_text = definition_text.split("[å¯èƒ½]")[1]

    return definition_text


def get_text_only_from_dictionary(
    word: str, reading: str, definition_data: list, dic_name: str
) -> str:
    """
    Extracts the main definition text from the raw data.

    Args:
    - definition_data (list): List of strings and possibly other data types containing the definition.
    - dic_name (str): Name of the dictionary being processed.

    Returns:
    - str: Cleaned and simplified definition text.
    """

    def get_non_recursive(information):
        """Iteratively extracts strings from the nested structure."""
        stack = [information]
        result = []

        while stack:
            current = stack.pop()
            if isinstance(current, str):
                """
                Add in a seperator, so later we can filter out 
                unwanted text easily.
                Make sure the seperator definitely won't appear 
                in the definitions
                """
                result.append(current)

            elif isinstance(current, list):
                flag = True
                """
                ä½¿ã„æ–¹ã®åˆ†ã‹ã‚‹ é¡èªä¾‹è§£è¾å…¸
                "content": [
                  {
                    "tag": "span",
                    "style": {
                      "fontWeight": "bold"
                    },
                    "content": "ãã†"
                  },
                  {
                    "tag": "span",
                    "style": {
                      "fontWeight": "normal"
                    },
                    "content": "ã€åƒ§ã€‘åƒ§ï¼åƒ§ä¾¶ï¼åŠä¸»ï¼åŠã•ã‚“ï¼å¾¡åŠï¼ãŠå¯ºã•ã‚“ï¼åƒ§å®¶ï¼æ²™é–€ï¼æ³•å¸«ï¼å‡ºå®¶ï¼æ¯”ä¸˜"
                  }
                ]
                ---------------------------------
                å®Ÿç”¨æ—¥æœ¬èªè¡¨ç¾è¾å…¸
                "content": [
                  {
                    "tag": "span",
                    "style": {
                      "fontWeight": "bold"
                    },
                    "content": "ã”ã˜ã‚ã„"
                  },
                  {
                    "tag": "span",
                    "style": {
                      "fontWeight": "normal"
                    },
                    "content": "ã€ã”è‡ªæ„›ã€‘"
                  }
                ]
                """

                if len(current) == 2:
                    first, second = current
                    # if dic_name.endswith("ä½¿ã„æ–¹ã®åˆ†ã‹ã‚‹ é¡èªä¾‹è§£è¾å…¸"):
                    #     reading_data, ruigigo_data = first, second

                    #     if "content" in ruigigo_data and "content" in reading:
                    #         if isinstance(ruigigo_data["content"], str):
                    #             actually_is_ruigigo = re.search(r"(.+?ï¼)+(.+)", ruigigo_data["content"])
                    #             if actually_is_ruigigo:
                    #                 # The actual é¡ç¾©èª part.
                    #                 flag = False

                    # These two are essentially the same thing.
                    if dic_name.endswith("å®Ÿç”¨æ—¥æœ¬èªè¡¨ç¾è¾å…¸") or dic_name.endswith(
                        "ä½¿ã„æ–¹ã®åˆ†ã‹ã‚‹ é¡èªä¾‹è§£è¾å…¸"
                    ):
                        if "content" in first:
                            if first["content"] == reading:
                                flag = False

                if flag:
                    stack.extend(reversed(current))  # Maintain original order

            elif isinstance(current, dict):
                flag = True
                content = current.get("content")
                if "data" in current:
                    if "name" in current["data"]:
                        current_name = current["data"]["name"]
                        # Some of these have a "G" suffix, some don't, etc.
                        unwanted_tags = re.compile(
                            "ã‚¢ã‚¯ã‚»ãƒ³ãƒˆ|è¡¨è¨˜|å“è©|ç”¨ä¾‹|æ³¨è¨˜|æ­´å²ä»®å|åŒºåˆ¥|ãƒ«ãƒ“|è¦‹å‡º|å¯èƒ½å½¢|ç•°å­—åŒè¨“"
                        )
                        if dic_name.endswith("å¤§è¾æ—"):
                            if (
                                current_name == "å˜ä½å"
                                and "content" in current["data"]
                            ):
                                if isinstance(current["data"]["content"], str):
                                    # (ã‚»ãƒ³ãƒãƒ¡ãƒ¼ãƒˆãƒ«)
                                    current["data"]["content"] = current["data"][
                                        "content"
                                    ][1:-1]
                            elif unwanted_tags.search(current_name):
                                flag = False

                        if (dic_name.endswith("ä½¿ã„æ–¹ã®åˆ†ã‹ã‚‹ é¡èªä¾‹è§£è¾å…¸")):
                            if current_name != "æ„å‘³":
                                flag = False

                        if dic_name.endswith("ä¸‰çœå ‚å›½èªè¾å…¸"):
                            """
                                {
                                  "tag": "span",
                                  "data": {
                                    "name": "å‚ç…§èªç¾©ç•ªå·"
                                  },
                                  "content": {
                                    "tag": "span",
                                    "data": {
                                      "name": "èªç¾©ç•ªå·"
                                    },
                                    "content": "â‘ "
                                  }
                                }
                            """

                            if unwanted_tags.search(current_name):
                                # Pretty sure ä¸‰çœå ‚å›½èªè¾å…¸ doesn't have this but å¤§è¾æ— does.
                                flag = False

                            elif "å‚ç…§èªç¾©ç•ªå·" in current_name:
                                if "content" in content:
                                    if isinstance(content["content"], str):
                                        reference_number = re.search(
                                            rf"^({NUMBER_CHARS})$", content["content"]
                                        )
                                        if reference_number:
                                            content["content"] = (
                                                f'({REFERENCE_NUMBER_MAP[content["content"]]})'
                                            )

                        # å®Ÿç”¨æ—¥æœ¬èªè¡¨ç¾è¾å…¸ doesn't seem to have any names other than "definition",
                        # but I put this here just in case.
                        if dic_name.endswith("å®Ÿç”¨æ—¥æœ¬èªè¡¨ç¾è¾å…¸"):
                            if current_name != "definition":
                                flag = False

                if dic_name.endswith("äº‹æ•…ãƒ»ã“ã¨ã‚ã–ãƒ»æ…£ç”¨å¥ã‚ªãƒ³ãƒ©ã‚¤ãƒ³"):
                    if "tag" in current:
                        tag = current["tag"]
                        """
                           Span example:
                          {
                            "tag": "span",
                            "content": "ã—ã®ã—ã‚‡ã†ã«ã‚“ã€æ­»ã®å•†äººã€‘"
                          },
                          Tables are are just the ç•°å½¢s summarized in table form.
                        """
                        if tag in ["span", "table"]:
                            flag = False

                if dic_name.endswith("ä¸‰çœå ‚å›½èªè¾å…¸"):
                    ...

                if flag:
                    if content:
                        stack.append(content)
            else:
                print(
                    f"Unexpected type encountered in dictionary '{dic_name}': {type(current)}"
                )  # Logging unexpected types
        return "".join(result)

    my_text = get_non_recursive(definition_data)

    return clean_definition(word, reading, my_text, dic_name)


def edit_big_data(big_data, dictionary_path, word, reading, definitions):
    # Given a word, its reading, and its definition, it creates a new datapoint
    # for said word/reading.

    """
    big_data = {
        "dictionary_path": {
            "word": {
                "reading1": ["definitions_1"],
                "reading2": ["definitions_2"],
            }
        }
    }
    """

    if word not in big_data[dictionary_path]:
        big_data[dictionary_path][word] = {}

    if reading not in big_data[dictionary_path][word]:
        big_data[dictionary_path][word][reading] = []

    if definitions:
        definitions = [x for x in definitions if "Weblio" not in x]
        big_data[dictionary_path][word][reading].extend(definitions)
        # Just in case there's dupelicates
        big_data[dictionary_path][word][reading] = list(
            set(big_data[dictionary_path][word][reading])
        )


def load_big_data(big_data_dictionary, override=False):
    if not override:
        with open("big_data.json", "r", encoding="utf-8") as f:
            return json.load(f)
    else:
        print("You're about to override big_data, continue?\ny\\N")
        user_choice = input()
        if user_choice != "y":
            exit()
        for dictionary_path in PRIORITY_ORDER:
            add_dictionary_to_big_data(dictionary_path, big_data_dictionary)

        # add_dictionary_to_big_data("6. æ—ºæ–‡ç¤¾å›½èªè¾å…¸ ç¬¬åä¸€ç‰ˆ", big_data_dictionary)
        # add_dictionary_to_big_data("4. ä½¿ã„æ–¹ã®åˆ†ã‹ã‚‹ é¡èªä¾‹è§£è¾å…¸", big_data_dictionary)
        # add_dictionary_to_big_data("8. Weblio", big_data_dictionary)

        # Write the final big_data to a JSON file
        save_to_big_data(big_data_dictionary)
        return big_data_dictionary


def save_to_big_data(big_data_dictionary):
    with open(BIG_DATA_FILE, "w", encoding="utf-8") as f:
        json.dump(big_data_dictionary, f, ensure_ascii=False, indent=2)
    print("Saved to big data")


if __name__ == "__main__":
    big_data_dictionary = load_big_data(big_data_dictionary, override=True)
