"""
Used to initially convert decks to
the big data.json file.
Has some usful functions and variables too
"""

import sys
import json
import re
import os
from scraper import convert_word_to_hiragana, get_hiragana_only

big_data_dictionary = {}
word_to_readings_map = {}
BIG_DATA_FILE = "big_data.json"

RED = "CC2222"
YELLOW = "ECE0B2"
GRAY = "808080"

PRIORITY_ORDER = [
    "æ•…äº‹ãƒ»ã“ã¨ã‚ã–ãƒ»æ…£ç”¨å¥ã‚ªãƒ³ãƒ©ã‚¤ãƒ³",
    "å®Ÿç”¨æ—¥æœ¬èªè¡¨ç¾è¾å…¸",
    "ä½¿ã„æ–¹ã®åˆ†ã‹ã‚‹ é¡èªä¾‹è§£è¾å…¸",
    "ä¸‰çœå ‚å›½èªè¾å…¸",
    "æ—ºæ–‡ç¤¾å›½èªè¾å…¸ ç¬¬åä¸€ç‰ˆ",
    "å¤§è¾æ³‰",
    "å¤§è¾æ—",
    "Weblio",
]
OPENING_BRACKETS = r"<ï¼ˆã€Œ\[ã€ã€”\(ã€ï¼»ã€ˆã€Šã€”ã€˜ï½Ÿ"
CLOSING_BRACKETS = r">ï¼‰ã€\]ã€‘ã€•\)ã€ï¼½ã€‰ã€‹ã€•ã€™ï½ "

KANSUUJI = ["ä¸€", "äºŒ", "ä¸‰", "å››", "äº”", "å…­", "ä¸ƒ", "å…«", "ä¹", "å"]

KANJI = r"\u3400-\u4dbf\u4e00-\u9fff\uf900-\ufaff\uff66-\uff9f"
HIRAGANA = r"ã‚-ã‚”"
KANA = r"ã‚-ãƒº"
NUMBER_CHARS = r"â‘ -â‘³â¶-â¿ã‰‘-ã‰Ÿâ‘´-â’‡â’ˆ-â’›âŠ-â“â€-â‰ğŸˆ©ğŸˆ”ğŸˆªãŠ€-ãŠ‰ãŠ¤ãŠ¥ãŠ¦ã‹-ã‹¾ï¼‘-ï¼™â“-â“©â’¶-â“ğŸ…-ğŸ…©"
FIRST_NUMBER_CHARS = r"â‘ â¶â‘´â’ˆâŠâ€ğŸˆ©ãŠ€ãŠ¤ã‹ï¼‘â“â’¶ğŸ…"
LAST_NUMBER_CHARS = r"â‘³â¿â‘³â’‡â’›â“â‰ğŸˆªãŠ‰ãŠ¦ã‹¾ï¼™â“©â“ğŸ…©"
NUMBERS_AND_EMOJIS = rf"[{NUMBER_CHARS}]|\dï¸âƒ£"
PREFIX = rf"{NUMBERS_AND_EMOJIS}|^|ã€‚|ãƒ»|<br />|\n|[{CLOSING_BRACKETS}{OPENING_BRACKETS}]| |ã€€|è¨˜å·.+?"
SUFFIX = rf"ã€‚|\n|<br ?/>|[{CLOSING_BRACKETS}{OPENING_BRACKETS}]| |ã€€"
ARROWS = r"â‡”â†’â†â˜â‡’â‡â‡¨"

NUMBER_CATEGORIES = {
    "â‘ ": "".join(chr(i) for i in range(ord("â‘ "), ord("â‘³") + 1))
    + "".join(chr(i) for i in range(ord("ã‰‘"), ord("ã‰Ÿ") + 1)),
    "â¶": "".join(chr(i) for i in range(ord("â¶"), ord("â¿") + 1)),
    "â‘´": "".join(chr(i) for i in range(ord("â‘´"), ord("â’‡") + 1)),
    "â’ˆ": "".join(chr(i) for i in range(ord("â’ˆ"), ord("â’›") + 1)),
    "âŠ": "".join(chr(i) for i in range(ord("âŠ"), ord("â“") + 1)),
    "â€": "".join(chr(i) for i in range(ord("â€"), ord("â‰") + 1)),
    "ğŸˆ©": "ğŸˆ©ğŸˆ”ğŸˆª",
    "ãŠ€": "".join(chr(i) for i in range(ord("ãŠ€"), ord("ãŠ‰") + 1)),
    "ãŠ¤": "ãŠ¤ãŠ¥ãŠ¦",
    "ã‹": "".join(chr(i) for i in range(ord("ã‹"), ord("ã‹¾") + 1)),
    # "ï¼‘": "".join(chr(i) for i in range(ord("ï¼"), ord("ï¼™") + 1)),
    "â“": "".join(chr(i) for i in range(ord("â“"), ord("â“©") + 1)),
    "â’¶": "".join(chr(i) for i in range(ord("â’¶"), ord("â“") + 1)),
    "ğŸ…": "".join(chr(i) for i in range(ord("ğŸ…"), ord("ğŸ…©") + 1)),
    "(1)": [f"({i})" for i in range(ord("1"), ord("9") + 1)],
    "KeyCapEmoji": [f"{i}ï¸âƒ£" for i in range(1, 10)],
}

NUMBER_CATEGORIES_REGEX = {
    "â‘ ": r"[â‘ -â‘³ã‰‘-ã‰Ÿ]+",
    "â¶": r"[â¶-â¿]+",
    "â‘´": r"[â‘´-â’‡]+",
    "â’ˆ": r"[â’ˆ-â’›]+",
    "âŠ": r"[âŠ-â“]+",
    "â€": r"[â€-â‰]+",
    "ğŸˆ©": r"[ğŸˆ©ğŸˆ”ğŸˆª]+",
    "ãŠ€": r"[ãŠ€-ãŠ‰]+",
    "ãŠ¤": r"[ãŠ¤-ãŠ¦]+",
    "ã‹": r"[ã‹-ã‹¾]+",
    # "ï¼‘": r"[ï¼-ï¼™]+",
    "â“": r"[â“-â“©]+",
    "â’¶": r"[â’¶-â“]+",
    "ğŸ…": r"[ğŸ…-ğŸ…©]+",
    "(1)": r"(\(\d+?\))+",
    "KeyCapEmoji": r"(?:\d+ï¸âƒ£)+",
}

REFERENCE_NUMBER_MAP = {
    **{f"({i})": i for i in range(1, 10)},
    **{chr(i): i - ord("â‘ ") + 1 for i in range(ord("â‘ "), ord("â‘³") + 1)},
    **{chr(i): i - ord("â‘´") + 1 for i in range(ord("â‘´"), ord("â’‡") + 1)},
    **{f"{i}ï¸âƒ£": i for i in range(1, 10)},
    **{chr(i): i - ord("â¶") + 1 for i in range(ord("â¶"), ord("â¿") + 1)},
    **{chr(i): i - ord("ã‰‘") + 21 for i in range(ord("ã‰‘"), ord("ã‰Ÿ") + 1)},
    **{chr(i): i - ord("ãŠ€") + 1 for i in range(ord("ãŠ€"), ord("ãŠ‰") + 1)},
    "ãŠ¤": "ä¸Š",
    "ãŠ¥": "ä¸­",
    "ãŠ¦": "ä¸‹",
    **{
        chr(i): chr(ord("ã‚¢") + (i - ord("ã‹")))
        for i in range(ord("ã‹"), ord("ã‹¾") + 1)
    },
    # **{chr(i): i - ord("ï¼") for i in range(ord("ï¼‘"), ord("ï¼™") + 1)},
    **{chr(i): chr(i - ord("â“") + ord("a")) for i in range(ord("â“"), ord("â“©") + 1)},
    **{chr(i): chr(i - ord("â’¶") + ord("A")) for i in range(ord("â’¶"), ord("â“") + 1)},
    **{chr(i): chr(i - ord("ğŸ…") + ord("A")) for i in range(ord("ğŸ…"), ord("ğŸ…©") + 1)},
}


def convert_reference_numbers(text):
    """Convert reference numbers in text to the format (number)."""

    # Function to replace each match with its mapped numeric value
    def replace_match(match):
        char = match.group(0)
        number = REFERENCE_NUMBER_MAP.get(char)
        return (
            f"ã€š{number}ã€›" if number else char
        )  # Return the number in parentheses or the char itself

    # Substitute each reference character with the desired format
    result = re.sub(
        r"|".join(map(re.escape, REFERENCE_NUMBER_MAP.keys())), replace_match, text
    )
    return result


def dict_to_text(d, level=0):
    """Convert a nested dictionary to a formatted string with indentation based on nesting level."""
    result = d["prefix"]

    for key, value in [(k, v) for k, v in d.items() if k != "prefix"]:
        if value in ["", ":", "\n"]:
            continue

        # Add a newline, then tabs based on the current level
        prefix = "â””" if level == 0 else "â””" + "â”€" * level
        result += "\n" + prefix + key

        # If the value is a string, add it after the key
        if isinstance(value, str):
            value = re.sub(r"^:|â””*$", "", value)
            result += " " + value
        # If the value is a nested dictionary, recursively convert it
        elif isinstance(value, dict):
            result += dict_to_text(value, level + 1)

    result = re.sub(r"(?:<br />|\n)+", r"\n", result)
    result = re.sub(
        rf"^(â””â”€*)({NUMBERS_AND_EMOJIS})â””â”€*({NUMBERS_AND_EMOJIS})", r"\1\2 \3 ", result
    )
    # a = result[:]
    result = re.sub(r"(?:â””â”€*)(?:\n|<br />|$)", "", result)
    return result


def find_first_category(text):
    """Identify the first number category that appears in the text."""
    first_category = None
    earliest_index = len(text) + 1  # Beyond bounds
    for category, pattern in NUMBER_CATEGORIES_REGEX.items():
        match_object = re.search(pattern, text)
        if match_object:
            start_index = match_object.span()[0]
            if start_index < earliest_index:
                earliest_index = start_index
                first_category = category
    return first_category


def segment_by_category(text, category, first_category, level):
    """
    Segments text by the first number characters of a specified category.
    If a key has a lower value than the previous or a jump of 2 or more,
    it includes that key and the rest of the segment in the key's segment.
    parameters:
        - text: str
        - category: str
        - first_category: str
        - level: int

    """

    # Get the pattern for the category and initialize tracking variables
    pattern = NUMBER_CATEGORIES_REGEX[category]
    category_regex = re.compile(f"({pattern[:-1]})")
    remove_prefixes = re.compile(r"â””â”€*$")

    def clean_text(string):
        return remove_prefixes.sub("", string).strip()

    segments_dict = {"prefix": ""}
    segments = category_regex.split(text)
    previous = 0  # Keep track of the last processed key's value
    previous_key = None
    i = 0
    while i < len(segments) - 1:
        try:
            if re.match(pattern, segments[i]):
                key = segments[i]
                current_number = NUMBER_CATEGORIES[category].index(key) + 1
                # Check if the current key is valid based on previous key's value

                is_referencing_other_level = level > 0 and first_category == category
                if is_referencing_other_level or (
                    current_number <= previous or current_number > previous + 1
                ):
                    # If the current key is lower or jumps 2 or more,
                    # we're talking about a different key in reference
                    segments_dict[previous_key] += key + clean_text(
                        "".join(segments[i + 1])
                    )
                else:
                    # Otherwise, add the segment normally
                    segments_dict[key] = clean_text(segments[i + 1])
                    previous = current_number  # Update highest
                    previous_key = key

                i += 2  # Move to the next potential key-value pair
            else:
                segments_dict["prefix"] += segments[i]
                i += 1  # Move to the next segment if not a key pattern match
        except ValueError:
            segments_dict[previous_key] += key
            if i + 1 < len(segments):
                segments_dict[previous_key] += clean_text("".join(segments[i + 1]))
            i += 1  # Move to the next segment if not a key pattern match

    return segments_dict


def recursive_nesting_by_category(
    text, first_category=None, next_category=None, level=0
):
    """Recursively separates the text into nested dictionaries by number character categories."""

    next_category = find_first_category(text)
    if not next_category:
        return text  # Base case: no number characters left
    if not first_category:
        first_category = next_category

    try:
        segments_dict = segment_by_category(
            text, first_category=first_category, category=next_category, level=level
        )
    except KeyError:
        return text  # Text, no longer has any segments

    for key, sub_text in segments_dict.items():
        # if not isinstance(sub_text, str):
        #     print(json.dumps(segments_dict, indent=2, ensure_ascii=False))
        #     print(sub_text)
        segments_dict[key] = recursive_nesting_by_category(
            sub_text,
            first_category=first_category,
            next_category=next_category,
            level=level + 1,
        )

    return segments_dict


def get_entry(ref_path, text):
    if not ref_path:
        return text

    entry_dict = recursive_nesting_by_category(text)
    if isinstance(entry_dict, str):
        return entry_dict  # Final destination

    current = entry_dict.copy()

    for step in ref_path:
        if isinstance(current, str):
            return current  # Final destination

        find_correct = [
            k
            for k in current.keys()
            if k != "prefix" and str(REFERENCE_NUMBER_MAP[k]) == step
        ]
        if find_correct:
            current = current[find_correct[0]]
        else:
            break

    if isinstance(current, str):
        return current  # Final destination
    if isinstance(current, dict):
        # Current is still a nested dictionary
        return dict_to_text(current)


def convert_to_path(reference_numbers):
    path = []
    counter = 0
    for i, x in enumerate(reference_numbers):
        if counter > 0:
            counter -= 1
            continue

        if x <= "9" and counter == 0:
            path.append(f"{x}{reference_numbers[i + 1]}{reference_numbers[i + 2]}")
            counter = 2
        else:
            path.append(x)

    return path


def add_dictionary_to_big_data(dictionary_path, big_data):
    """
    Adds words and their definitions from dictionary files to the global `big_data` dictionary.

    Args:
    - dictionary_path (str): Path to the dictionary folder.
    - big_data (dict): The shared dictionary.
    """
    print(f"Adding from {dictionary_path}")
    term_bank_files = sorted(
        [
            f
            for f in os.listdir(dictionary_path)
            if re.match(r"term_bank_\d+\.json$", f)
        ],
        key=lambda x: int(re.search(r"\d+", x).group()),
    )

    for file in term_bank_files:
        # data = None
        # with open(f"æ—ºæ–‡ç¤¾å›½èªè¾å…¸ ç¬¬åä¸€ç‰ˆ/{file}", "r", encoding="utf-8") as f:
        #     data = json.load(f)

        # with open(f"æ—ºæ–‡ç¤¾å›½èªè¾å…¸ ç¬¬åä¸€ç‰ˆ/{file}", "w", encoding="utf-8") as f:
        #     json.dump(data, f, indent=2, ensure_ascii=False)

        process_term_bank_file(file, dictionary_path, big_data)


def process_term_bank_file(file, dictionary_path, big_data):
    """Processes a single term bank file."""
    print(f"Processing {file} in {dictionary_path}")

    if dictionary_path not in big_data:
        big_data[dictionary_path] = {}

    file_path = os.path.join(dictionary_path, file)
    words_to_remove = []

    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
            for entry in data:
                word, reading, entry_type, definitions_in_data = (
                    entry[0],
                    entry[1],
                    entry[2],
                    entry[5],
                )
                # Skip entries with unwanted types
                if entry_type not in ["å­", "å¥"]:
                    # Handle missing or convert reading to Hiragana
                    if not reading:
                        reading = get_hiragana_only(word)
                    else:
                        reading = get_hiragana_only(reading)

                    definition_list = []
                    for definition in definitions_in_data:
                        definition_text = get_text_only_from_dictionary(
                            word, reading, definition, dictionary_path
                        )
                        if definition_text:
                            definition_list.append(definition_text)

                else:
                    definition_list = []

                word = word.replace("ï¼", "")
                if not definition_list:  # No definitions for entry?
                    words_to_remove.append(word)
                else:
                    # Update call to `edit_big_data` with the new structure
                    edit_big_data(
                        big_data, dictionary_path, reading, word, definition_list
                    )

    except Exception as e:
        print(f"Error processing file {file}: {e}")
        raise e


def edit_big_data(big_data, dictionary_path, reading, word, definitions):
    """
    Updates big_data with the specified structure:

    big_data = {
        "dictionary_path": {
            "reading": {
                "word1": ["definitions_1"],
                "word2": ["definitions_2"],
            }
        }
    }
    """
    if re.search(r"^\d+$", word):
        print(f"Skipping all-number word {word}")
        return
    # Ensure dictionary_path and reading exist
    if reading not in big_data[dictionary_path]:
        big_data[dictionary_path][reading] = {}

    # Add definitions to the word under the reading
    if word not in big_data[dictionary_path][reading]:
        big_data[dictionary_path][reading][word] = []

    # Remove definitions containing "Weblio" and add the rest
    filtered_definitions = [x for x in definitions if "Weblio" not in x]

    big_data[dictionary_path][reading][word].extend(filtered_definitions)

    # Ensure unique definitions for the word
    big_data[dictionary_path][reading][word] = list(
        set(big_data[dictionary_path][reading][word])
    )
    # if reading == "ã‹ã‹ã‚Šã‚€ã™ã³":
    # print(word)
    if word not in word_to_readings_map:
        word_to_readings_map[word] = []

    word_to_readings_map[word].append(reading)
    word_to_readings_map[word] = list(set(word_to_readings_map[word]))


def replace_furigana_references(text):
    # hiragana_kanji_references = re.finditer(
    #         rf"([{HIRAGANA}]+?)(?:ï¼ˆ| \()((?:(?:[{KANJI}]+)(?:[{HIRAGANA}]+))+)(?:ï¼ˆ|\) )", text
    #     )
    # if hiragana_kanji_references:
    #     for r in hiragana_kanji_references:
    #         the_match = r.group()
    #         the_hiragana = r.group(1)
    #         the_kanji = r.group(2)

    text = text.replace("ï¼ˆ", " (").replace("ï¼‰", ") ")
    a_prefix = rf"({PREFIX})?"
    words_and_furigana = rf"((?:([{KANJI}]+)(?: \([{HIRAGANA}]+)\) ?)+)([{HIRAGANA}]+)?"
    a_suffix = rf"((?:{NUMBERS_AND_EMOJIS})+)?"
    ref_with_furigana = re.compile(
        rf"{a_prefix}â‡’{words_and_furigana}{a_suffix}",
        flags=re.UNICODE,
    )

    match_object = ref_with_furigana.finditer(text)

    # links = []

    if match_object:
        for match in match_object:
            # Since we can't just "guess" the kanji's readings,
            # I'm only taking reading into account when it
            # describes the entire word.

            reading_match = re.search(
                rf"( \(([{HIRAGANA}]+)\) ?)(?:(?:[{NUMBER_CHARS}]|(\dï¸âƒ£))+|\n|$)",
                match.group(),
            )
            has_kanji = re.search(rf"[{KANJI}]", match.group())
            furigana = None

            if reading_match and has_kanji:
                matched = reading_match.group(1)
                furigana = "".join(
                    ["".join([y if y else "" for y in x]) for x in matched]
                )

            no_furigana_and_ref = re.findall(
                rf"[a-zA-Z]|[{KANJI}]|(?:{NUMBERS_AND_EMOJIS})+$|[^(â‡’][{HIRAGANA}]+[^)]?$",
                match.group(),
                flags=re.U,
            )

            # number = match.groups()[3] if match.groups()[3] else ""

            if no_furigana_and_ref:
                no_furigana_and_ref = "".join(
                    [x.replace(" ", "") for x in no_furigana_and_ref]
                )

            # No furigana that describes the entire word
            # â†“
            # Replace with the no-furigana version

            # Has furigana that describes the entire word
            # â†“
            # Don't replace

            original = f"{match.group(2)}{match.group(4) if match.group(4) else ''}"
            if not furigana:
                text = text.replace(original, no_furigana_and_ref)

    return text


def normalize_references(text: str, dictionary_path: str) -> str:
    text = re.sub(rf" ?[{ARROWS}]", "â‡’", text)
    text = text.replace("\\n", "\n")
    text = re.sub(r"<br ?/>", "\n", text)
    flag = False
    text_original = text[:]

    if dictionary_path.endswith("å¤§è¾æ³‰"):
        # ï¼»åï¼½(ã‚¹ãƒ«)ã€Œã‚¢ãƒ«ãƒã‚¤ãƒˆã€ã®ç•¥ã€‚ã€Œå¤ä¼‘ã¿ã«ãƒã‚¤ãƒˆã™ã‚‹ã€
        text = re.sub(rf"â‡’", " â‡’", text)
        has_ryaku = re.compile(
            rf"({PREFIX})ã€Œ([^{OPENING_BRACKETS}]+?)ã€ã®ç•¥({SUFFIX})"
        )
        if has_ryaku.search(text):
            adding_text = has_ryaku.sub(r"â‡’\2ã€‚ ", has_ryaku.search(text).group())
            if adding_text not in text:
                text += "\n" + adding_text

        # Handle this?
        # ï¼»é€£èªï¼½â‡’ç½® (ãŠ) ãâ‘«
        # â‡’äººè¿” (ã²ã¨ãŒãˆ) ã—â‘¡

        """
        0-11  â‡’ç•°åŒ– (ã„ã‹) â‘¡
        0-1  
        2-10  ç•°åŒ– (ã„ã‹) 
        2-4   ç•°åŒ–
        10-11 â‘¡
        """

        # flag = True
        text = replace_furigana_references(text)
        text = re.sub(r"â‡’â‡’+", "â‡’", text)
        text = text.replace("\\n", "\n")
        # convert ã«åŒã˜ format to â‡’ format for linking purposes later.
        # Either in the beginning, between lines, or between periods.
        #                             Prefix   Word        Suffix
        definition_text = re.sub(
            rf"({PREFIX})ã€Œ(.+?) \((.+?)\) ã€ã«åŒã˜({SUFFIX})", r"\1â‡’\3 (\2) \4 ", text
        )
        definition_text = re.sub(
            rf"({PREFIX})ã€Œ(.+?)ã€ã«åŒã˜({SUFFIX})", r"\1â‡’\2\3 ", text
        )

        # ã€‚ã€Œè¨€è‘‰â‘ ã€ã«åŒã˜ã€‚
        # ã€‚â‡’è¨€è‘‰â‘ 
        # ã€‚â‡’è¨€è‘‰(1) (Later)

        # ã€‚â‡’å†…åŒ å¯® (ãŸãã¿ã‚Šã‚‡ã†) â‘ 
        # ã€‚â‡’IOAï¼ˆIndependent Olympic Athletesï¼‰
        # â‡’ã‚³ãƒãƒ¼ã‚·ãƒ£ãƒ«â‘ 
        # ...ãƒ©ãƒãƒ¼ã€‚â†’å¼¾æ€§ã‚´ãƒ \nâ‘¡ æ¤ç‰©ã‹ã‚‰...
        # â‡’é‰±å·¥æ¥­ç”Ÿç”£æŒ‡æ•°â‘ 
        pattern_text = rf"({PREFIX})?â‡’([^\n]+?)(?:ï¼ˆ.+?ï¼‰)?((?:{NUMBERS_AND_EMOJIS})*?)({SUFFIX}|$|ãƒ»| |ã€€)"
        pattern = re.compile(pattern_text)

        results = pattern.finditer(text)

        if results:
            # if len([x for x in results]) > 1:

            for result in results:
                _prefix, word, reference_number, suffix = result.groups()

                _prefix = _prefix if _prefix else ""
                reference_number = reference_number if reference_number else ""
                suffix = suffix if suffix else ""
                # Reference number
                reference_numbers = convert_to_path(reference_number)
                try:
                    references = "".join(
                        [convert_reference_numbers(x) for x in reference_numbers]
                    )
                except KeyError:
                    print("[ERROR]\t", text, reference_numbers)

                text = pattern.sub(f"{_prefix}â‡’{word}{reference_number}{suffix}", text)

    if dictionary_path.endswith("ä¸‰çœå ‚å›½èªè¾å…¸"):
        definition_text = re.sub(
            rf"({PREFIX})ã€Œ(.+?) \((.+?)\) ã€ã¨åŒã˜({SUFFIX})", r"\1â‡’\3 (\2) \4 ", text
        )
        definition_text = re.sub(
            rf"({PREFIX})ã€Œ(.+?)ã€ã¨åŒã˜({SUFFIX})", r"\1â‡’\2\3 ", text
        )

        # â‡’ã€Œ.+?ã€
        reference_pattern = rf"â‡’ã€Œ(.+?)((?:{NUMBERS_AND_EMOJIS})+)ã€"

        in_kagigakko = re.compile(reference_pattern)
        if in_kagigakko.search(text):
            text = in_kagigakko.sub(r"ã€Œâ‡’\1\2ã€", text)

        # â‡’è„‡â‘¦ãƒ»æŒ™ã’å¥â‘¡ã€‚
        # â†“
        # â‡’è„‡â‘¦ã€€â‡’æŒ™ã’å¥â‘¡ã€‚
        reference_pattern = (
            rf"([^\nï¼šãƒ»{NUMBER_CHARS}\dï¸âƒ£â‡’ {OPENING_BRACKETS}{CLOSING_BRACKETS}]+)"
            rf"( \(.+?\) ?)?((?:[{NUMBER_CHARS}\dï¸âƒ£]| ?\(\d+\) ?)*)"
        )

        pattern_mulitple = re.compile(
            rf"â‡’{reference_pattern}((?:(?:ï¼š|ãƒ»){reference_pattern})+)($|\n| |ã€€|ã€‚|[{CLOSING_BRACKETS}])"
        )

        results_multiple = pattern_mulitple.finditer(text)

        text_original = text[:]
        if results_multiple:
            for i, result in enumerate(results_multiple):
                first = i == 1  #                          Remove â‡’
                references = re.split("ãƒ»", result.group()[1:])
                result_original = result.group()
                result_after_changes = ""
                for reference in references:
                    reference = (
                        reference.strip("\n").strip(" ").strip("ã€€").replace("ï½ ", "")
                    )
                    fixed_nubmers_reference = re.sub(
                        r" ?\((d+)\) ?", r"ã€š{\1}ã€›", reference
                    )
                    result_after_changes = re.sub(
                        rf" ?(?:â‡’|ãƒ»){re.escape(reference)}",
                        rf" â‡’{fixed_nubmers_reference} ",
                        result_original,
                    )
                text = text.replace(result_original, result_after_changes)

        text = replace_furigana_references(text)

        # â‘ æœã€‚åˆå‰ã€‚            â˜“
        # â‘¡ã€˜æœã€™â†ãƒ¢ãƒ¼ãƒ‹ãƒ³ã‚°ã‚³ãƒ¼ãƒˆã€‚ â—¯
        # â‘¢â†ãƒ¢ãƒ¼ãƒ‹ãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹ã€‚ã€€ã€€ â—¯
        # (!) Remeber,   All arrows are now "â‡’"

        pattern = re.compile(
            rf"({NUMBERS_AND_EMOJIS}|^|ã€‚|\n|[{CLOSING_BRACKETS}{OPENING_BRACKETS}]| |ã€€|è¨˜å·.+?)â‡’([{NUMBER_CHARS}]*)([^\d{OPENING_BRACKETS}]+?)([{NUMBER_CHARS}]|\dï¸)?(ãƒ»|$|ã€‚|<br />|\n)"
        )
        results = pattern.finditer(text)
        for result in results:
            _prefix, _, word, reference_number, suffix = result.groups()
            _prefix = _prefix if _prefix else ""
            reference_number = reference_number if reference_number else ""
            suffix = suffix if suffix else ""
            text = pattern.sub(f"{_prefix}â‡’{word}{reference_number}{suffix} ", text)

    if dictionary_path.endswith("å¤§è¾æ—"):
        # ï¼»åï¼½(ã‚¹ãƒ«)ã€Œã‚¢ãƒ«ãƒã‚¤ãƒˆã€ã®ç•¥ã€‚ã€Œå¤ä¼‘ã¿ã«ãƒã‚¤ãƒˆã™ã‚‹ã€
        text = text.replace(" ãƒ»", "ãƒ»")
        has_ryaku = re.compile(
            rf"({PREFIX})ã€Œ([^{OPENING_BRACKETS}]+?)ã€ã®ç•¥({SUFFIX})"
        )
        if has_ryaku.search(text):
            adding_text = has_ryaku.sub(r"â‡’\2ã€‚ ", has_ryaku.search(text).group())
            if adding_text not in text:
                text += "\n" + adding_text

        reference_pattern = rf"([^\nãƒ»{NUMBER_CHARS}\dï¸âƒ£â‡’ {OPENING_BRACKETS}{CLOSING_BRACKETS}]+)( \(.+?\) ?)?((?:[{NUMBER_CHARS}\dï¸âƒ£]| ?\(\d+\) ?)*)"

        pattern_mulitple = re.compile(
            rf"â‡’{reference_pattern}((?:ãƒ»{reference_pattern})+)($|\n| |ã€€|ã€‚|[{CLOSING_BRACKETS}])"
        )

        results_multiple = pattern_mulitple.finditer(text)

        text_original = text[:]
        if results_multiple:
            for i, result in enumerate(results_multiple):
                first = i == 1  #                          Remove â‡’
                references = re.split("ãƒ»", result.group()[1:])
                result_original = result.group()
                result_after_changes = ""
                for reference in references:
                    reference = (
                        reference.strip("\n").strip(" ").strip("ã€€").replace("ï½ ", "")
                    )
                    fixed_nubmers_reference = re.sub(
                        r" ?\((d+)\) ?", r"ã€š{\1}ã€›", reference
                    )
                    result_after_changes = re.sub(
                        rf" ?(?:â‡’|ãƒ»){re.escape(reference)}",
                        rf" â‡’{fixed_nubmers_reference} ",
                        result_original,
                    )
                text = text.replace(result_original, result_after_changes)

        text = replace_furigana_references(text)

    if dictionary_path.endswith("ä½¿ã„æ–¹ã®åˆ†ã‹ã‚‹ é¡èªä¾‹è§£è¾å…¸"):
        ...

    if dictionary_path.endswith("æ—ºæ–‡ç¤¾å›½èªè¾å…¸ ç¬¬åä¸€ç‰ˆ"):
        # Change it out of our format. Not a reference
        # ã€Œï½˜â‡’ï½˜â‡’ã€
        text = text.replace("ï¼ˆ", " (").replace("ï¼‰", ") ")
        transformations = re.finditer(r"ã€Œ(?:.+?â‡’)+(?:.+?)ã€", text)
        for transformation in transformations:
            text = text.replace(
                transformation.group(), transformation.group().replace("â‡’", "â†’")
            )

        text = re.sub(r" ?â‡’", " â‡’", text)

        # â‡’ã‘ã‚“ï¼ˆçŒ®ï¼‰  -  Hiragana (kanji)
        hiragana_kanji_references = re.finditer(
            rf"([{HIRAGANA}]+?)(?:ï¼ˆ| \()((?:(?:[{KANJI} ]+)(?:[{HIRAGANA}]+))+)(?:ï¼ˆ|\) )",
            text,
        )
        if hiragana_kanji_references:
            for r in hiragana_kanji_references:
                the_match = r.group()
                the_hiragana = r.group(1).replace(" ", "")
                the_kanji = r.group(2).replace(" ", "")
                text = text.replace(
                    the_match, f"{the_kanji.replace(' ', '')} ({the_hiragana}) "
                )

        text = replace_furigana_references(text)

        text = re.sub(rf"â‡’([{HIRAGANA}]+) \(([{HIRAGANA}]+)\)", "â‡’\1\2", text)
        # ã“ã‚Œã‹ã‚‰èµ·ã“ã‚‹äº‹æŸ„ã‚’è¡¨ã™è¨€ã„ æ–¹ã€‚<br />ï½Ÿ â‡’éå»ãƒ»ç¾åœ¨ï½ "

        # â‡’å¤äºº(1)ï¼šå¤äºº(2)
        # â†“
        # â‡’å¤äºº(1) â‡’å¤äºº(2)ã€‚

        # â‡’ä¸‹ãŒã‚‹ â‡’ãŠã‚Šã‚‹ (ä¸‹ã‚Šã‚‹) (1)ï¼šãŠã‚Šã‚‹ (é™ã‚Šã‚‹) (2)
        reference_pattern = rf"([^\nï¼šãƒ»{NUMBER_CHARS}\dï¸âƒ£â‡’ {OPENING_BRACKETS}{CLOSING_BRACKETS}]+)( \(.+?\) ?)?((?:[{NUMBER_CHARS}\dï¸âƒ£]| ?\(\d+\) ?)*)"

        pattern_mulitple = re.compile(
            rf"â‡’{reference_pattern}((?:(?:ï¼š|ãƒ»){reference_pattern})+)($|\n| |ã€€|ã€‚|[{CLOSING_BRACKETS}])"
        )

        results_multiple = pattern_mulitple.finditer(text)

        text_original = text[:]
        if results_multiple:
            for i, result in enumerate(results_multiple):
                first = i == 1  #                          Remove â‡’
                references = re.split("ï¼š|ãƒ»", result.group()[1:])
                result_original = result.group()
                result_after_changes = ""
                for reference in references:
                    reference = (
                        reference.strip("\n").strip(" ").strip("ã€€").replace("ï½ ", "")
                    )
                    fixed_nubmers_reference = re.sub(
                        r" ?\((d+)\) ?", r"ã€š{\1}ã€›", reference
                    )
                    result_after_changes = re.sub(
                        rf" ?(?:â‡’|ï¼š|ãƒ»){re.escape(reference)}",
                        f" â‡’{fixed_nubmers_reference} ",
                        result_original,
                    )
                text = text.replace(result_original, result_after_changes)

        # â‡’è¨€èªï¼ˆã’ã‚“ã”ï¼‰- Gengo (Furigana)
        # Change full-width brackets to half-width for later function
        text = re.sub(rf"ï¼ˆ([{HIRAGANA}]+?)ï¼‰", rf" (\1) ", text)
        text = text = replace_furigana_references(text)

        if text.endswith("\nâ‡’ã€Œä½¿ã„åˆ†ã‘ã€"):
            text = text[: -len("\nâ‡’ã€Œä½¿ã„åˆ†ã‘ã€")]

    text = text.replace(" â‡’", "â‡’")
    text = re.sub(rf"ãƒ»(?:[{NUMBER_CHARS}]|\dï¸âƒ£)", "", text)

    # Search for reference pattern in the definition
    reference_matches = re.finditer(
        rf"â‡’([^(]+?)( \([ã‚-ã‚”]+\) )?((?:{NUMBERS_AND_EMOJIS})*)(?:ã€‚|$|\n|<br />| |ã€€)",
        text,
    )

    # {prefix}{tag}â‡’{word}{references}{suffix}
    # already_linked = []
    # If there's a reference in the definition
    if reference_matches:
        for reference_match in reference_matches:
            last_char = reference_match.group()[-1]
            suffix = last_char if last_char in ["ã€‚", "\n", "ã€€", " ", ";"] else ""
            if suffix == ";" and reference_match.group().endswith("<br />"):
                suffix = "<br />"

            referenced_word, furigana, reference_number_path = reference_match.groups()
            furigana = furigana if furigana else ""

            reference_number_path = (
                reference_number_path if reference_number_path else ""
            )
            reference_numbers = convert_to_path(reference_number_path)

            try:
                reference_numbers = "".join(
                    [convert_reference_numbers(x) for x in reference_numbers]
                )
            except KeyError:
                print("[ERROR]\t", text, reference_numbers)

            text = text.replace(
                reference_match.group(),
                f" â‡’{referenced_word}{furigana}{reference_numbers} ",
            )
            text += suffix

    return text.replace("\n", "<br />")


def clean_definition(
    word: str, reading: str, definition_text: str, dictionary_path: str
) -> str:
    # ã€”â‘¢ãŒåŸç¾©ã€• !todo
    """
    Cleans and formats the definition text based on the specific dictionary.

    Args:
    - definition_text (str): The raw definition text to clean.
    - dictionary_path (str): The name or identifier of the dictionary.

    Returns:
    - str: The cleaned and formatted definition text.
    """
    # Remove the first line for specific dictionaries

    my_word = word == "1"

    if word.endswith("ã®è§£èª¬"):
        return None
    # Normalize \n's
    definition_text = definition_text.replace("\\n", "\n")
    # Weird character
    definition_text = definition_text.replace("â€‰", " ")
    definition_text = definition_text.split("Linked")[0]
    # Unecessary parts
    definition_text = re.sub(
        r"(?:\[è£œèª¬\]|ï¼»è£œèª¬ï¼½|ï¼»ç”¨æ³•ï¼½|\[ç”¨æ³•\]|\[å¯èƒ½\]|ï¼»å¯èƒ½ï¼½)(?:.|\n)+",
        "",
        definition_text,
    ).strip()

    # I don't even know why this appears at times
    definition_text = definition_text.replace("_x000D_", "")
    definition_text = definition_text.replace("\r", "")
    definition_text = definition_text.replace("\1", "")
    definition_text = definition_text.replace("\2", "")

    if not word and not reading:
        return None

    # Normalize spaces after numbers:
    definition_text = re.sub(
        rf"((?:{NUMBERS_AND_EMOJIS})[ ]+)+",
        r"\1".replace(" ", "") + " ",
        definition_text,
    )

    definition_text = normalize_references(definition_text, dictionary_path)

    # if "â‡’" in definition_text:

    # Using endswith because I don't care about their order in the priority (or what order you chose to give them
    # in the folder name). Just matters that it ends with the dictionary name.
    if my_word:
        print(1, definition_text)
    if dictionary_path.endswith("å¤§è¾æ³‰"):
        splitted = definition_text.split("<br />")
        if len(splitted) > 1:
            definition_text = "<br />".join(splitted[1:])  # Remove first line

        if "[å¯èƒ½]" in definition_text:
            definition_text = definition_text.split("[å¯èƒ½]")[0]
        if "[æ´¾ç”Ÿ]" in definition_text:
            definition_text = definition_text.split("[æ´¾ç”Ÿ]")[0]
        # Remove remains of example sentences
        # â‘£: ç´å¾—ã™ã‚‹ã€‚åˆç‚¹ãŒã„ãã€‚ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ» (after parsing)
        definition_text = re.sub(r"ãƒ»(?:ãƒ»|ï¼)+", "", definition_text)
        definition_text = re.sub(r"ã€‚ã€‚+", "ã€‚", definition_text)

        # ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ï¼ãƒ»ã€‚ ã€‚
        # ï¼»å‹•ã‚¶ä¸Šä¸€ï¼½ã€Œã¾ã‚“ï¼ˆæ…¢ï¼‰ãšã‚‹ã€ï¼ˆã‚µå¤‰ï¼‰ã®ä¸Šä¸€æ®µåŒ–ã€‚
        # ï¼»å‹•ã‚¶ä¸Šä¸€ï¼½ã€Œã¿ãã‚“ãšã‚‹ã€ï¼ˆã‚µå¤‰ï¼‰ã®ä¸Šä¸€æ®µåŒ–ã€‚ã€Œè©±é¡Œã®å±•è¦§ä¼šã‚’â€•ãƒ»ã˜ã‚‹ã€
        # ï¼»å‹•ã‚¶ä¸Šä¸€ï¼½ã€Œã¦ã‚“ï¼ˆè»¢ï¼‰ãšã‚‹ã€ï¼ˆã‚µå¤‰ï¼‰ã®ä¸Šä¸€æ®µåŒ–ã€‚ã€Œæ”»å‹¢ã«â€•ãƒ»ã˜ã‚‹ã€

        # First fix ã€Œã¦ã‚“ï¼ˆè»¢ï¼‰ãšã‚‹ã€ â†’ "ã€Œè»¢ãšã‚‹ã€"
        definition_text = re.sub(
            rf"ã€Œ[{HIRAGANA}]+ï¼ˆ([{KANJI}]+)ï¼‰([{HIRAGANA}]+)ã€",
            r"ã€Œ\1\2ã€",
            definition_text,
        )

        # Then fix ï¼»å‹•ã‚¶ä¸Šä¸€ï¼½ã€Œè»¢ãšã‚‹ã€ï¼ˆã‚µå¤‰ï¼‰ã®ä¸Šä¸€æ®µåŒ–ã€‚ã€Œæ”»å‹¢ã«â€•ãƒ»ã˜ã‚‹ã€ â†’  "â‡’è»¢ãšã‚‹"
        definition_text = re.sub(
            rf"(?:ï¼».+?ï¼½)ã€Œ(.+?)ã€ã®(?:..?æ®µåŒ–|..èª)({SUFFIX})",
            r"â‡’\1\2 ",
            definition_text,
        )

        # Remove
        # ï¼»é€£èªï¼½ã€Šå½¢å®¹è©ã€ãŠã‚ˆã³å½¢å®¹è©å‹æ´»ç”¨èªã®é€£ä½“å½¢æ´»ç”¨èªå°¾ã€Œã‹ã‚‹ã€ã«æ¨é‡ã®åŠ©å‹•è©ã€Œã‚ã‚Šã€ã®ä»˜ã„ãŸã€Œã‹ã‚‹ã‚ã‚Šã€ã®éŸ³å¤‰åŒ–ã€‹
        # ï¼»é€£èªï¼½ã€Šé€£èªã€Œã‹ã‚“ã‚ã‚Šã€ã®æ’¥éŸ³ã®ç„¡è¡¨è¨˜ã€‹
        definition_text = re.sub(r"ï¼».+?ï¼½ã€Š.+?ã€‹", r"", definition_text)

        # ï¼»å‹•ãƒ©ä¸‹ä¸€ï¼½ï¼»æ–‡ï¼½ã‹ãã¿ã ãƒ»ã‚‹ï¼»ãƒ©ä¸‹äºŒï¼½
        # ï¼»å‹•ãƒ©äº”ï¼ˆå››ï¼‰ï¼½
        # ï¼»å‹•ã‚µä¸‹ä¸€ï¼½ï¼»æ–‡ï¼½ã‹ãã‚ˆãƒ»ã™ï¼»ã‚µä¸‹äºŒï¼½
        # ï¼»åï¼½(ã‚¹ãƒ«)
        # ï¼»å½¢å‹•ï¼½ï¼»æ–‡ï¼½ï¼»ãƒŠãƒªï¼½
        definition_text = re.sub(
            rf"(?:ï¼».+?ï¼½)+(?:[{HIRAGANA}ãƒ»]+ï¼».+?ï¼½)?(?:\(ã‚¹ãƒ«\))?",
            r"",
            definition_text,
        )

        # ã€Œä¸€ã¤æ±²ã‚“ã§ä¸‹ã•ã‚Œã¨ã€ä¸‹ã€…ã«ã‚‚â€•ã«è© (ã“ã¨ã°) é£ã²ã¦ã€ã€ˆæµ®ãƒ»ç¦çŸ­æ°—ãƒ»äºŒã€‰
        definition_text = re.sub(
            rf"ã€Œ[^ã€]+?ã€ã€ˆ[^ã€‰]+?ã€‰",
            r"",
            definition_text,
        )
        # ã€Šå­£ æ–°å¹´ã€‹ã€Œé¤…ç¶²ã‚‚ç„¦ã’ã¦â€•ã¨ãªã‚Šã«ã‘ã‚Šï¼å‹äºŒã€
        definition_text = re.sub(
            rf"ã€Š[^ã€‹]+?ã€‹ã€Œ[^ã€]+?ã€",
            r"",
            definition_text,
        )

    if my_word:
        print(2, definition_text)

    if dictionary_path.endswith("æ—ºæ–‡ç¤¾å›½èªè¾å…¸ ç¬¬åä¸€ç‰ˆ"):
        definition_text = definition_text.replace("ã€”é•ã„ã€•", "")
        # (å½¢) ã€Šã‚«ãƒ­ãƒ»ã‚«ãƒ„ (ã‚¯) ãƒ»ã‚¤ãƒ»ã‚¤ãƒ»ã‚±ãƒ¬ãƒ»â—‹ã€‹

        definition_text = re.sub(r" ?\(.+?\) ã€Š.+?ã€‹", "", definition_text)

        # Remove first line
        # ã‚ã„â€ã—ã‚‡ã†ã€å“€å‚·ã€‘â€•â€•ã‚·ãƒ¤ã‚¦\n
        splitted = definition_text.split("<br />")
        if len(splitted) > 1:
            definition_text = "<br />".join(splitted[2:])  # Remove first line

        # Remove the first line in items like this.
        # ã‚ã„ã€æŒ¨ã€‘\nã‚¢ã‚¤ãŠ¥\nãŠã™\nç­†é †ï¼š\n
        # \n\nï¼ˆå­—ç¾©ï¼‰\nâ‘  ãŠã™ã€‚æŠ¼ã—ã®ã‘ã‚‹ã€‚ã€ŒæŒ¨æ‹¶ï¼ˆã‚ã„ã•ã¤ï¼‰ï¼ˆï¼åŸç¾©ã¯æŠ¼ã—ã®ã‘ã¦é€²ã‚€æ„ã€‚å›½ ...

        if "ç­†é †ï¼š" in definition_text:
            return None
            # definition_text = definition_text.split("ç­†é †ï¼š")[1]
        if "å­—ç¾©" in definition_text:
            return None
            # definition_text = definition_text.split("(å­—ç¾©)")[1]

        definition_text = re.sub(r"å›³ç‰ˆï¼š\n?", "", definition_text)
        definition_text = definition_text.strip("<br />")

        # Remove
        # ï¼ˆåãƒ»ä»–ã‚¹ãƒ«ï¼‰\n.
        # ï¼ˆå½¢ï¼‰ã€Šã‚«ãƒ­ãƒ»ã‚«ãƒ„ï¼ˆã‚¯ï¼‰ãƒ»ã‚¤ãƒ»ã‚¤ãƒ»ã‚±ãƒ¬ãƒ»â—‹ã€‹\n
        # But keep (â€¦ã®ç•¥) ?

        definition_text = re.sub(r"ï¼ˆ.+?(?!ã®ç•¥)ï¼‰(ã€Š.+?ã€‹)?\n", "", definition_text)

        # Remove
        # ã€”å¯èƒ½ã€•ã‚ãŒãƒ»ã‚Œã‚‹ï¼ˆä¸‹ä¸€ï¼‰<br />
        # ã€”ä»–ã€•ã‚ãƒ»ã’ã‚‹ï¼ˆä¸‹ä¸€ ï¼‰
        definition_text = re.sub(
            rf"ã€”.+?ã€•?[{HIRAGANA}ãƒ»]+ï¼ˆ.+?ï¼‰({SUFFIX})", r"\1", definition_text
        )

        # Remove everything after ã€˜ä½¿ã„åˆ†ã‘ã€™
        if "ã€˜ä½¿ã„åˆ†ã‘ã€™" in definition_text:
            definition_text = definition_text.split("ã€˜ä½¿ã„åˆ†ã‘ã€™")[0]

        # Remove everything after ã€˜ã¡ãŒã„ã€™
        if "ã€˜ã¡ãŒã„ã€™" in definition_text:
            definition_text = definition_text.split("ã€˜ã¡ãŒã„ã€™")[0]

    if dictionary_path.endswith("ä½¿ã„æ–¹ã®åˆ†ã‹ã‚‹ é¡èªä¾‹è§£è¾å…¸"):
        ...
        # This is already handled in the scraping function.
        # definition_text = definition_text.split(r'ğŸ“šä½¿ã„æ–¹')[0]
        # definition_text = definition_text.split(r'ğŸ”„ä½¿ã„åˆ†ã‘')[0]
        # definition_text = definition_text.split(r'ğŸ”—é–¢é€£èª')[0]

    if dictionary_path.endswith("ä¸‰çœå ‚å›½èªè¾å…¸"):
        definition_text = re.sub(r"ã€”ã€•", "", definition_text)
        ...
        # This is already handled in the scraping function
        # definition_text = re.sub(r"^.+?ï½ <br />|ã€Œ.+ã€(?:<br />)?", "", definition_text)

    if dictionary_path.endswith("äº‹æ•…ãƒ»ã“ã¨ã‚ã–ãƒ»æ…£ç”¨å¥ã‚ªãƒ³ãƒ©ã‚¤ãƒ³"):
        ...
        # This is already handled in the scraping function

        # Remove spans like this
        # ã—ã‚Šã¦ã—ã‚‰ã–ã‚Œã€çŸ¥ã‚Šã¦çŸ¥ã‚‰ã–ã‚Œã€‘
        # ã€å¤±æ•—ã¯æˆåŠŸã®ã‚‚ã¨ã€‘

    if dictionary_path.endswith("å¤§è¾æ—"):
        no_period_quote = re.search(r"[^ã€‚ã€]$", definition_text)
        final_word_reference = re.search(
            rf"â‡’[{KANJI}{KANA}a-zA-Zãƒ»]+$", definition_text
        )
        if no_period_quote and not final_word_reference:
            return None
        definition_text = definition_text.split("è£œèª¬æ¬„")[0]

        # ã€”â‘¢ãŒåŸç¾©ã€• ......
        startswith_comment = re.sub(r"^ã€”.+?ã€•", "", definition_text)

    if dictionary_path.endswith("å®Ÿç”¨æ—¥æœ¬èªè¡¨ç¾è¾å…¸"):
        definition_text = re.sub(f"^{re.escape(word)}", "", definition_text)

        # This is already handled in the scraping function

    if dictionary_path.endswith("Weblio"):
        # ï¼»å‹•ã‚«ä¸‹ä¸€ï¼½ï¼»æ–‡ï¼½ãªã¤ãƒ»ãï¼»ã‚«ä¸‹äºŒï¼½ã€Šã€Œãªã¥ã‘ã‚‹ã€ã¨ã‚‚ã€‹
        # ï¼»å‹•ã‚¢ä¸‹ä¸€ï¼½ï¼»æ–‡ï¼½ã‹ã¾ãƒ»ãµï¼»ãƒä¸‹äºŒï¼½
        # ï¼»å‹•ã‚«äº”ï¼ˆå››ï¼‰ï¼½
        definition_text = re.sub(
            rf"({PREFIX})(?:ï¼».+?ï¼½)+(?:[{HIRAGANA}ãƒ»]+ï¼».+?ï¼½)?(?:ã€Š.+?ã€‹)?({SUFFIX})",
            r"\1\2",
            definition_text,
        )
        # ï¼»åï¼½(ã‚¹ãƒ«)
        # ï¼»å½¢å‹•ï¼½ï¼»æ–‡ï¼½ï¼»ãƒŠãƒªï¼½
        definition_text = re.sub(
            rf"(?:ï¼».+?ï¼½)+(?:[{HIRAGANA}ãƒ»]+ï¼».+?ï¼½)?(?:\(ã‚¹ãƒ«\))?",
            r"",
            definition_text,
        )

    # # Add line breaks before entry numbers
    # definition_text = re.sub(rf"({NUMBERS_AND_EMOJIS})", r"<br />\1", definition_text)
    # Clean up leading or trailing unwanted characters

    if definition_text:
        definition_text = definition_text.strip("\n").strip("<br />")
        # once

    # if "â‡’" in definition_text:
    #     definition_text = re.sub(rf"({PREFIX})â‡’([{NUMBER_CHARS}]*)(.+)($|ã€‚|<br />|\n)", r"\1\2\3\4", definition_text)

    # Normalize numbers back
    definition_text = re.sub(rf"([{NUMBER_CHARS}][^ ]) ", r"\1 ", definition_text)
    # if "éŠé‡Œã§å®¢ã®ç›¸æ‰‹ã¨ãªã‚‹éŠå¥³" in definition_text:

    # Normalize line breaks
    definition_text = definition_text.replace("\n", "<br />").replace("\\n", "<br />")

    # Contract multiple linebreaks into a single linebreak
    # For some fucking reason {2,} doesn't work so here we are.
    definition_text = re.sub(r"(<br />|\n|\\n)+", r"<br />", definition_text)

    # if "éŠé‡Œã§å®¢ã®ç›¸æ‰‹ã¨ãªã‚‹éŠå¥³" in definition_text:

    # Temp
    # definition_text = definition_text.replace("<br />", "\n")

    # if "éŠé‡Œã§å®¢ã®ç›¸æ‰‹ã¨ãªã‚‹éŠå¥³" in definition_text:
    if my_word:
        print(3, definition_text)
    definition_dict = recursive_nesting_by_category(definition_text)
    if isinstance(definition_dict, dict):
        definition_text = dict_to_text(definition_dict)
    else:
        definition_text = definition_dict

    definition_text = re.sub(r"ã€‚+", "ã€‚", definition_text)
    definition_text = re.sub(r" *ã€‚", "ã€‚", definition_text)

    definition_text = re.sub(r"(?: |ã€€)+", " ", definition_text)

    definition_text = definition_text.strip("\n").strip()

    if "[å¯èƒ½]" in definition_text:
        definition_text = definition_text.split("[å¯èƒ½]")[1]

    if my_word:
        print(4, definition_text)
    return definition_text.replace("\n", "<br />")


def get_text_only_from_dictionary(
    word: str, reading: str, definition_data: list, dic_name: str
) -> str:
    """
    Extracts the main definition text from the raw data.

    Args:
    - definition_data (list): List of strings and possibly other data types containing the definition.
    - dic_name (str): Name of the dictionary being processed.

    Returns:
    - str: Cleaned and simplified definition text.
    """

    def get_non_recursive(information):
        """Iteratively extracts strings from the nested structure."""
        stack = [information]
        result = []

        while stack:
            current = stack.pop()
            if isinstance(current, str):
                # reference_number = re.search(
                #     rf"^({NUMBER_CHARS})$", current
                # )
                # if reference_number:
                #     current = (
                #         f'ã€š{REFERENCE_NUMBER_MAP[current]}ã€›'
                #     )
                result.append(current)

            elif isinstance(current, list):
                flag = True
                """
                ä½¿ã„æ–¹ã®åˆ†ã‹ã‚‹ é¡èªä¾‹è§£è¾å…¸
                "content": [
                  {
                    "tag": "span",
                    "style": {
                      "fontWeight": "bold"
                    },
                    "content": "ãã†"
                  },
                  {
                    "tag": "span",
                    "style": {
                      "fontWeight": "normal"
                    },
                    "content": "ã€åƒ§ã€‘åƒ§ï¼åƒ§ä¾¶ï¼åŠä¸»ï¼åŠã•ã‚“ï¼å¾¡åŠï¼ãŠå¯ºã•ã‚“ï¼åƒ§å®¶ï¼æ²™é–€ï¼æ³•å¸«ï¼å‡ºå®¶ï¼æ¯”ä¸˜"
                  }
                ]
                ---------------------------------
                å®Ÿç”¨æ—¥æœ¬èªè¡¨ç¾è¾å…¸
                "content": [
                  {
                    "tag": "span",
                    "style": {
                      "fontWeight": "bold"
                    },
                    "content": "ã”ã˜ã‚ã„"
                  },
                  {
                    "tag": "span",
                    "style": {
                      "fontWeight": "normal"
                    },
                    "content": "ã€ã”è‡ªæ„›ã€‘"
                  }
                ]
                """

                if len(current) == 2:
                    first, _ = current
                    # if dic_name.endswith("ä½¿ã„æ–¹ã®åˆ†ã‹ã‚‹ é¡èªä¾‹è§£è¾å…¸"):
                    #     reading_data, ruigigo_data = first, second

                    #     if "content" in ruigigo_data and "content" in reading:
                    #         if isinstance(ruigigo_data["content"], str):
                    #             actually_is_ruigigo = re.search(r"(.+?ï¼)+(.+)", ruigigo_data["content"])
                    #             if actually_is_ruigigo:
                    #                 # The actual é¡ç¾©èª part.
                    #                 flag = False

                    # These two are essentially the same thing.
                    if dic_name.endswith("å®Ÿç”¨æ—¥æœ¬èªè¡¨ç¾è¾å…¸") or dic_name.endswith(
                        "ä½¿ã„æ–¹ã®åˆ†ã‹ã‚‹ é¡èªä¾‹è§£è¾å…¸"
                    ):
                        if "content" in first:
                            if first["content"] == reading:
                                flag = False

                if flag:
                    stack.extend(reversed(current))  # Maintain original order

            elif isinstance(current, dict):
                flag = True
                content = current.get("content")
                if "data" in current:
                    if "name" in current["data"]:
                        current_name = current["data"]["name"]
                        # Some of these have a "G" suffix, some don't, etc.
                        unwanted_tags = re.compile(
                            "é•ã„|æ´¾ç”Ÿ|åŒºåˆ¥|ç™¾ç§‘|ã‚¢ã‚¯ã‚»ãƒ³ãƒˆ|è¡¨è¨˜|å“è©|ç”¨ä¾‹|å¯¾ç¾©èª"
                            "æ³¨è¨˜|æ­´å²ä»®å|åŒºåˆ¥|ãƒ«ãƒ“|è¦‹å‡º|å¯èƒ½å½¢|ç•°å­—åŒè¨“"
                        )
                        if dic_name.endswith("å¤§è¾æ—"):
                            if (
                                current_name == "å˜ä½å"
                                and "content" in current["data"]
                            ):
                                if isinstance(current["data"]["content"], str):
                                    # (ã‚»ãƒ³ãƒãƒ¡ãƒ¼ãƒˆãƒ«)
                                    current["data"]["content"] = current["data"][
                                        "content"
                                    ][1:-1]
                            elif unwanted_tags.search(current_name):
                                flag = False

                        if dic_name.endswith("ä½¿ã„æ–¹ã®åˆ†ã‹ã‚‹ é¡èªä¾‹è§£è¾å…¸"):
                            if current_name != "æ„å‘³":
                                flag = False

                        if dic_name.endswith("ä¸‰çœå ‚å›½èªè¾å…¸"):
                            """
                                {
                                  "tag": "span",
                                  "data": {
                                    "name": "å‚ç…§èªç¾©ç•ªå·"
                                  },
                                  "content": {
                                    "tag": "span",
                                    "data": {
                                      "name": "èªç¾©ç•ªå·"
                                    },
                                    "content": "â‘ "
                                  }
                                }
                            """
                            """
                                {
                                    "tag": "span",
                                    "data": {
                                        "name": "å‚ç…§èªç¾©ç•ªå·"
                                    },
                                    "content": {
                                        "tag": "span",
                                        "style": {
                                            "verticalAlign": "text-bottom",
                                            "marginRight": 0.25
                                        },
                                        "content": {
                                            "tag": "img",
                                            "height": 1.0,
                                            "width": 1.0,
                                            "sizeUnits": "em",
                                            "appearance": "auto",
                                            "title": "äºŒ",
                                            "collapsible": false,
                                            "collapsed": false,
                                            "background": false,
                                            "path": "sankoku8/äºŒ-bluefill.svg"
                                        }
                                    }
                                }   
                            """
                            """
                                "tag": "div",
                                "data": {
                                    "name": "å¤§èªç¾©"
                                },
                                "content": [
                                    {
                                        "tag": "span",
                                        "style": {
                                            "verticalAlign": "text-bottom",
                                            "marginRight": 0.25
                                        },
                                        "content": {
                                            "tag": "img",
                                            "height": 1.0,
                                            "width": 1.0,
                                            "sizeUnits": "em",
                                            "appearance": "monochrome",
                                            "title": "äºŒ",
                                            "collapsible": false,
                                            "collapsed": false,
                                            "background": false,
                                            "path": "sankoku8/äºŒ-fill.svg"
                                        }
                                    }
                                ]
                            """
                            # print(current_name)
                            if unwanted_tags.search(current_name):
                                # Pretty sure ä¸‰çœå ‚å›½èªè¾å…¸ doesn't have this but å¤§è¾æ— does.
                                flag = False

                            elif "å‚ç…§èªç¾©ç•ªå·" in current_name:
                                if "content" in content:
                                    if isinstance(content["content"], str):
                                        reference_number = re.search(
                                            rf"^({NUMBER_CHARS})$", content["content"]
                                        )
                                        if reference_number:
                                            content["content"] = (
                                                f'ã€š{REFERENCE_NUMBER_MAP[content["content"]]}ã€›'
                                            )
                                        current["content"] = content

                        if dic_name.endswith("å¤§è¾æ³‰"):
                            if unwanted_tags.search(current_name):
                                flag = False
                        # å®Ÿç”¨æ—¥æœ¬èªè¡¨ç¾è¾å…¸ doesn't seem to have any names other than "definition",
                        # but I put this here just in case.
                        if dic_name.endswith("å®Ÿç”¨æ—¥æœ¬èªè¡¨ç¾è¾å…¸"):
                            if current_name != "definition":
                                flag = False

                if dic_name.endswith("äº‹æ•…ãƒ»ã“ã¨ã‚ã–ãƒ»æ…£ç”¨å¥ã‚ªãƒ³ãƒ©ã‚¤ãƒ³"):
                    if "tag" in current:
                        tag = current["tag"]
                        """
                           Span example:
                          {
                            "tag": "span",
                            "content": "ã—ã®ã—ã‚‡ã†ã«ã‚“ã€æ­»ã®å•†äººã€‘"
                          },
                          Tables are are just the ç•°å½¢s summarized in table form.
                        """
                        if tag in ["span", "table"]:
                            flag = False

                if dic_name.endswith("å¤§è¾æ—"):
                    if "title" in current:
                        title = current["title"]
                        if title in KANSUUJI:
                            content = f"{KANSUUJI.index(title) + 1}ï¸âƒ£"

                if dic_name.endswith("ä¸‰çœå ‚å›½èªè¾å…¸"):
                    """
                        "content": {
                            "tag": "img",
                            "height": 1.0,
                            "width": 1.0,
                            "sizeUnits": "em",
                            "appearance": "monochrome",
                            "title": "äºŒ",
                            "collapsible": false,
                            "collapsed": false,
                            "background": false,
                            "path": "sankoku8/äºŒ-fill.svg"
                        }                        
                    """
                    if "title" in current:
                        title = current["title"]
                        if title in KANSUUJI:
                            content = f"{KANSUUJI.index(title) + 1}ï¸âƒ£"
                if flag:
                    if content:
                        stack.append(content)
            else:
                print(
                    f"Unexpected type encountered in dictionary '{dic_name}': {type(current)}"
                )  # Logging unexpected types
        return "".join(result)

    my_text = get_non_recursive(definition_data)

    return clean_definition(word, reading, my_text, dic_name)


# def edit_big_data(big_data, dictionary_path, word, reading, definitions):
#     # Given a word, its reading, and its definition, it creates a new datapoint
#     # for said word/reading.

#     """
#     big_data = {
#         "dictionary_path": {
#             "word": {
#                 "reading1": ["definitions_1"],
#                 "reading2": ["definitions_2"],
#             }
#         }
#     }
#     """

#     if word not in big_data[dictionary_path]:
#         big_data[dictionary_path][word] = {}

#     if reading not in big_data[dictionary_path][word]:
#         big_data[dictionary_path][word][reading] = []

#     if definitions:
#         definitions = [x for x in definitions if "Weblio" not in x]
#         big_data[dictionary_path][word][reading].extend(definitions)
#         # Just in case there's dupelicates
#         big_data[dictionary_path][word][reading] = list(
#             set(big_data[dictionary_path][word][reading])
#         )


def load_big_data(big_data_dictionary, override=False):
    if not override:
        with open("big_data.json", "r", encoding="utf-8") as f:
            return json.load(f)
    else:
        print("You're about to override big_data, continue?\ny\\N")
        user_choice = input()
        if user_choice != "y":
            sys.exit()
        for dictionary_path in PRIORITY_ORDER:
            print(f"Loading {dictionary_path}")
            add_dictionary_to_big_data(dictionary_path, big_data_dictionary)

        # add_dictionary_to_big_data("æ—ºæ–‡ç¤¾å›½èªè¾å…¸ ç¬¬åä¸€ç‰ˆ", big_data_dictionary)
        # add_dictionary_to_big_data("ä½¿ã„æ–¹ã®åˆ†ã‹ã‚‹ é¡èªä¾‹è§£è¾å…¸", big_data_dictionary)
        # add_dictionary_to_big_data("Weblio", big_data_dictionary)

        # Write the final big_data to a JSON file
        save_to_big_data(big_data_dictionary)
        return big_data_dictionary


def save_to_big_data(big_data_dictionary):
    with open(BIG_DATA_FILE, "w", encoding="utf-8") as f:
        json.dump(big_data_dictionary, f, ensure_ascii=False, indent=2)
    with open("word_to_readings_map.json", "w", encoding="utf-8") as f:
        json.dump(word_to_readings_map, f, ensure_ascii=False, indent=2)
    print("Saved to big data")


if __name__ == "__main__":
    big_data_dictionary = load_big_data(big_data_dictionary, override=True)
