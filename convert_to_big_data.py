import pandas as pd
import json
import re
import os
from json.decoder import JSONDecodeError
big_data_dictionary = {}
from scraper import convert_word_to_hiragana
BIG_DATA_FILE = "big_data.json"
PRIORITY_ORDER = [
    "1. å¤§è¾æ³‰",   
    "7. ä¸‰çœå ‚å›½èªè¾å…¸",
    "5. æ•…äº‹ãƒ»ã“ã¨ã‚ã–ãƒ»æ…£ç”¨å¥ã‚ªãƒ³ãƒ©ã‚¤ãƒ³",
    "2. å®Ÿç”¨æ—¥æœ¬èªè¡¨ç¾è¾å…¸",
    "3. å¤§è¾æ—",
    # "4. ä½¿ã„æ–¹ã®åˆ†ã‹ã‚‹ é¡èªä¾‹è§£è¾å…¸",
    # "6. æ—ºæ–‡ç¤¾å›½èªè¾å…¸ ç¬¬åä¸€ç‰ˆ",
    # "8. Weblio",
]   
OPENING_BRACKETS = r"ï¼ˆã€Œ\[ã€ã€”\(ã€ï¼»ã€ˆã€Šã€”ã€˜"
CLOSING_BRACKETS = r"ï¼‰ã€\]ã€‘ã€•\)ã€ï¼½ã€‰ã€‹ã€•ã€™"

KANJI = fr"\u3400-\u4dbf\u4e00-\u9fff\uf900-\ufaff\uff66-\uff9f"
HIRAGANA = fr"ã‚-ã‚”"
NUMBER_CHARS = r"â‘ -â‘³â¶-â¿ã‰‘-ã‰Ÿâ‘´-â’‡â’ˆ-â’›âŠ-â“â€-â‰ğŸˆ©ğŸˆ”ğŸˆªãŠ€-ãŠ‰ãŠ¤ãŠ¥ãŠ¦ã‹-ã‹¾ï¼‘-ï¼™â“-â“©â’¶-â“ğŸ…-ğŸ…©"
FIRST_NUMBER_CHARS = r"â‘ â¶â‘´â’ˆâŠâ€ğŸˆ©ãŠ€ãŠ¤ã‹ï¼‘â“â’¶ğŸ…"
LAST_NUMBER_CHARS = r"â‘³â¿â‘³â’‡â’›â“â‰ğŸˆªãŠ‰ãŠ¦ã‹¾ï¼™â“©â“ğŸ…©"
PREFIX = fr"[{NUMBER_CHARS}]|\dï¸âƒ£|^|ã€‚|<br\/>&nbsp;|\n|[{CLOSING_BRACKETS}{OPENING_BRACKETS}]| |ã€€|è¨˜å·.+?"
SUFFIX = fr"ã€‚|\n|<br\/>&nbsp;"
ARROWS = fr"â‡”â†’â†â˜â‡’â‡â‡¨"
NUMBER_CATEGORIES = {
    'â‘ ': r"[â‘ -â‘³ã‰‘-ã‰Ÿ]+",
    'â¶': r"[â¶-â¿]+",
    'â‘´': r"[â‘´-â’‡]+",
    'â’ˆ': r"[â’ˆ-â’›]+",
    'âŠ': r"[âŠ-â“]+",
    'â€': r"[â€-â‰]+",
    'ğŸˆ©': r"[ğŸˆ©ğŸˆ”ğŸˆª]",
    'ãŠ€': r"[ãŠ€-ãŠ‰]+",
    'ãŠ¤': r"[ãŠ¤-ãŠ¦]+",
    'ã‹': r"[ã‹-ã‹¾]+",
    'ï¼‘': r"[ï¼-ï¼™]+",
    'â“': r"[â“-â“©]+",
    'â’¶': r"[â’¶-â“]+",
    'ğŸ…': r"[ğŸ…-ğŸ…©]+",
    "KeyCapEmoji": r"(?:\d+ï¸âƒ£)+"
}

# Maybe there's a smarter way to do this but lololol
REFERENCE_NUMBER_MAP = {
    # Circled Numbers
    'â‘ ': 1, 'â‘¡': 2, 'â‘¢': 3, 'â‘£': 4, 'â‘¤': 5, 'â‘¥': 6, 'â‘¦': 7, 'â‘§': 8, 'â‘¨': 9, 'â‘©': 10,
    'â‘ª': 11, 'â‘«': 12, 'â‘¬': 13, 'â‘­': 14, 'â‘®': 15, 'â‘¯': 16, 'â‘°': 17, 'â‘±': 18, 'â‘²': 19, 'â‘³': 20,
    
    # Parenthesized Numbers
    'â‘´': 1, 'â‘µ': 2, 'â‘¶': 3, 'â‘·': 4, 'â‘¸': 5, 'â‘¹': 6, 'â‘º': 7, 'â‘»': 8, 'â‘¼': 9, 'â‘½': 10,
    'â‘¾': 11, 'â‘¿': 12, 'â’€': 13, 'â’': 14, 'â’‚': 15, 'â’ƒ': 16, 'â’„': 17, 'â’…': 18, 'â’†': 19, 'â’‡': 20,
    "1ï¸âƒ£": 1, "2ï¸âƒ£": 2, "3ï¸âƒ£": 3, "4ï¸âƒ£": 4, "5ï¸âƒ£": 5, "6ï¸âƒ£": 6, "7ï¸âƒ£": 7, "8ï¸âƒ£": 8, "9ï¸âƒ£": 9,
    # Double Circled Numbers
    'â¶': 1, 'â·': 2, 'â¸': 3, 'â¹': 4, 'âº': 5, 'â»': 6, 'â¼': 7, 'â½': 8, 'â¾': 9, 'â¿': 10,
    
    # Enclosed Alphanumeric Supplement
    'ã‰‘': 21, 'ã‰’': 22, 'ã‰“': 23, 'ã‰”': 24, 'ã‰•': 25, 'ã‰–': 26, 'ã‰—': 27, 'ã‰˜': 28, 'ã‰™': 29, 'ã‰š': 30,
    'ã‰›': 31, 'ã‰œ': 32, 'ã‰': 33, 'ã‰': 34, 'ã‰Ÿ': 35,
    
    # Enclosed CJK Ideographic Supplement
    'ãŠ€': 1, 'ãŠ': 2, 'ãŠ‚': 3, 'ãŠƒ': 4, 'ãŠ„': 5, 'ãŠ…': 6, 'ãŠ†': 7, 'ãŠ‡': 8, 'ãŠˆ': 9, 'ãŠ‰': 10,
    
    # Enclosed CJK Ideographic Units
    'ãŠ¤': 'ä¸Š', 'ãŠ¥': 'ä¸­', 'ãŠ¦': 'ä¸‹',
    
    # Enclosed Katakana
    'ã‹': 'ã‚¢', 'ã‹‘': 'ã‚¤', 'ã‹’': 'ã‚¦', 'ã‹“': 'ã‚¨', 'ã‹”': 'ã‚ª', 'ã‹•': 'ã‚«', 'ã‹–': 'ã‚­', 'ã‹—': 'ã‚¯', 'ã‹˜': 'ã‚±', 'ã‹™': 'ã‚³',
    'ã‹š': 'ã‚µ', 'ã‹›': 'ã‚·', 'ã‹œ': 'ã‚¹', 'ã‹': 'ã‚»', 'ã‹': 'ã‚½', 'ã‹Ÿ': 'ã‚¿', 'ã‹ ': 'ãƒ', 'ã‹¡': 'ãƒ„', 'ã‹¢': 'ãƒ†', 'ã‹£': 'ãƒˆ',
    'ã‹¤': 'ãƒŠ', 'ã‹¥': 'ãƒ‹', 'ã‹¦': 'ãƒŒ', 'ã‹§': 'ãƒ', 'ã‹¨': 'ãƒ', 'ã‹©': 'ãƒ', 'ã‹ª': 'ãƒ’', 'ã‹«': 'ãƒ•', 'ã‹¬': 'ãƒ˜', 'ã‹­': 'ãƒ›',
    'ã‹®': 'ãƒ', 'ã‹¯': 'ãƒŸ', 'ã‹°': 'ãƒ ', 'ã‹±': 'ãƒ¡', 'ã‹²': 'ãƒ¢', 'ã‹³': 'ãƒ¤', 'ã‹´': 'ãƒ¦', 'ã‹µ': 'ãƒ¨', 'ã‹¶': 'ãƒ©', 'ã‹·': 'ãƒª',
    'ã‹¸': 'ãƒ«', 'ã‹¹': 'ãƒ¬', 'ã‹º': 'ãƒ­', 'ã‹»': 'ãƒ¯', 'ã‹¼': 'ãƒ°', 'ã‹½': 'ãƒ±', 'ã‹¾': 'ãƒ²',
    
    # Fullwidth Numbers
    'ï¼‘': 1, 'ï¼’': 2, 'ï¼“': 3, 'ï¼”': 4, 'ï¼•': 5, 'ï¼–': 6, 'ï¼—': 7, 'ï¼˜': 8, 'ï¼™': 9,
    # 'ï¼‘ï¼‘': 11, 'ï¼‘ï¼’': 12, 'ï¼‘ï¼“': 13, 'ï¼‘ï¼”': 14, 'ï¼‘ï¼•': 15, 'ï¼‘ï¼–': 16, 'ï¼‘ï¼—': 17, 'ï¼‘ï¼˜': 18, 'ï¼‘ï¼™': 19,
    # 'ï¼’ï¼': 20,
    # Enclosed Alphanumeric
    'â“': 'a', 'â“‘': 'b', 'â“’': 'c', 'â““': 'd', 'â“”': 'e', 'â“•': 'f', 'â“–': 'g', 'â“—': 'h', 'â“˜': 'i', 'â“™': 'j',
    'â“š': 'k', 'â“›': 'l', 'â“œ': 'm', 'â“': 'n', 'â“': 'o', 'â“Ÿ': 'p', 'â“ ': 'q', 'â“¡': 'r', 'â“¢': 's', 'â“£': 't',
    'â“¤': 'u', 'â“¥': 'v', 'â“¦': 'w', 'â“§': 'x', 'â“¨': 'y', 'â“©': 'z',
    
    # Enclosed Latin Letters
    'â’¶': 'A', 'â’·': 'B', 'â’¸': 'C', 'â’¹': 'D', 'â’º': 'E', 'â’»': 'F', 'â’¼': 'G', 'â’½': 'H', 'â’¾': 'I', 'â’¿': 'J',
    'â“€': 'K', 'â“': 'L', 'â“‚': 'M', 'â“ƒ': 'N', 'â“„': 'O', 'â“…': 'P', 'â“†': 'Q', 'â“‡': 'R', 'â“ˆ': 'S', 'â“‰': 'T',
    'â“Š': 'U', 'â“‹': 'V', 'â“Œ': 'W', 'â“': 'X', 'â“': 'Y', 'â“': 'Z',
    
    # Fullwidth Latin Letters (uppercase)
    'ğŸ…': 'A', 'ğŸ…‘': 'B', 'ğŸ…’': 'C', 'ğŸ…“': 'D', 'ğŸ…”': 'E', 'ğŸ…•': 'F', 'ğŸ…–': 'G', 'ğŸ…—': 'H', 'ğŸ…˜': 'I', 'ğŸ…™': 'J',
    'ğŸ…š': 'K', 'ğŸ…›': 'L', 'ğŸ…œ': 'M', 'ğŸ…': 'N', 'ğŸ…': 'O', 'ğŸ…Ÿ': 'P', 'ğŸ… ': 'Q', 'ğŸ…¡': 'R', 'ğŸ…¢': 'S', 'ğŸ…£': 'T',
    'ğŸ…¤': 'U', 'ğŸ…¥': 'V', 'ğŸ…¦': 'W', 'ğŸ…§': 'X', 'ğŸ…¨': 'Y', 'ğŸ…©': 'Z'
}

def convert_reference_numbers(text):
    """Convert reference numbers in text to the format (number)."""
    
    # Function to replace each match with its mapped numeric value
    def replace_match(match):
        char = match.group(0)
        number = REFERENCE_NUMBER_MAP.get(char)
        return f"ã€š{number}ã€›" if number else char  # Return the number in parentheses or the char itself
    
    # Substitute each reference character with the desired format
    result = re.sub(r'|'.join(map(re.escape, REFERENCE_NUMBER_MAP.keys())), replace_match, text)
    return result


def dict_to_text(d, level=0):
    """Convert a nested dictionary to a formatted string with indentation based on nesting level."""
    result = ""
    
    for key, value in d.items():
        if value in ["", ":"]:
            continue
        # Add a newline, then tabs based on the current level
        prefix = "â””" if level == 0 else "â””" + "â”€" * level
        result += "\n" + prefix + key
        
        # If the value is a string, add it after the key
        if isinstance(value, str):
            value = re.sub(r"^:", "", value)
            result += " " + value
        # If the value is a nested dictionary, recursively convert it
        elif isinstance(value, dict):
            result += dict_to_text(value, level + 1)

    result = re.sub(fr"(?:<br/>&nbsp;){2,}", r"<br/>&nbsp;", result)
    result = re.sub(fr"\n{2,}", r"\n", result)

    return result

    
def find_first_category(text):
    """Identify the first number category that appears in the text."""
    first_category = None  
    earliest_index = len(text)+1  # Beyond bounds
    for category, pattern in NUMBER_CATEGORIES.items():

        match_object = re.search(pattern, text)
        if match_object:
            start_index = match_object.span()[0]
            if start_index < earliest_index:
                earliest_index = start_index
                first_category = category
    
    return first_category

def segment_by_category(text, category):
    """Segments text by the first number characters of a specified category."""
    pattern = NUMBER_CATEGORIES[category]
    segments = re.split(f"({pattern})", text)
    # Create dictionary with segments, using number chars as keys
    segments_dict = {}
    i = 0
    while i < len(segments) - 1:
        if re.match(pattern, segments[i]):
            key = segments[i]
            segments_dict[key] = segments[i + 1].strip()
            i += 2
        else:
            i += 1
    return segments_dict

def recursive_nesting_by_category(text):
    """Recursively separates the text into nested dictionaries by number character categories."""
    first_category = find_first_category(text)
    if not first_category:
        return text  # Base case: no number characters left

    # Segment by the first identifier
    segments_dict = segment_by_category(text, first_category)

    # Recursively process each segment
    for key, sub_text in segments_dict.items():
        segments_dict[key] = recursive_nesting_by_category(sub_text)
    
    return segments_dict


def get_entry(ref_path, text, big_data):
    entry_dict = recursive_nesting_by_category(text)

    current = entry_dict.copy()

    for step in ref_path:
        find_correct = [k for k in current.keys() if str(REFERENCE_NUMBER_MAP[k]) == step]
        if find_correct:
            current = current[find_correct[0]]
        else:
            raise KeyError(f"Could not find the equivalent to {step} in {current.keys()}")    

    if isinstance(current, str):
        return current  # Final destination
    elif isinstance(current, dict):
        # Current is still a nested dictionary
        text = convert_to_text(current)

        return convert_to_text(current)



def convert_to_path(reference_numbers):
    path = []
    counter = 0
    for i, x in enumerate(reference_numbers):
        
        if counter > 0:
            counter -= 1
            continue

        if x <= '9' and counter == 0:
            path.append(f"{x}{reference_numbers[i + 1]}{reference_numbers[i + 2]}")
            counter = 2
        else:
            path.append(x)

    return path



def add_dictionary_to_big_data(dictionary_path, big_data):
    """
    Adds words and their definitions from dictionary files to the global `big_data` dictionary.

    Args:
    - dictionary_path (str): Path to the dictionary folder.
    - big_data (dict): The shared dictionary.
    """
    print(f"Adding from {dictionary_path}")
    term_bank_files = sorted(
        [f for f in os.listdir(dictionary_path) if re.match(r'term_bank_\d+\.json$', f)],
        key=lambda x: int(re.search(r'\d+', x).group())
    )

    for file in term_bank_files:
        # data = None
        # with open(f"6. æ—ºæ–‡ç¤¾å›½èªè¾å…¸ ç¬¬åä¸€ç‰ˆ/{file}", "r", encoding="utf-8") as f:
        #     data = json.load(f)

        # with open(f"6. æ—ºæ–‡ç¤¾å›½èªè¾å…¸ ç¬¬åä¸€ç‰ˆ/{file}", "w", encoding="utf-8") as f:
        #     json.dump(data, f, indent=2, ensure_ascii=False)

        process_term_bank_file(file, dictionary_path, big_data)

def process_term_bank_file(file, dictionary_path, big_data):
    """Processes a single term bank file."""
    print(f"Processing {file} in {dictionary_path}")

    if dictionary_path not in big_data:
        big_data[dictionary_path] = {}

    file_path = os.path.join(dictionary_path, file)
    words_to_remove = []

    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
            # if dictionary_path.endswith("å¤§è¾æ—"):
            #     data = data[0]
            for i, entry in enumerate(data):
                word, reading, definitions_in_data = entry[0], entry[1], entry[5]
                
                # Words can have different readings, and different definitions.
                # æœ€ä¸­ï¼ˆã•ã„ã¡ã‚…ã†ãƒ»ã•ãªã‹ï¼‰etc. 
                # If a word has multiple definitions for a single reading, run over all of them.
                # We are currently handling a single entry.

                if not reading:
                    reading = convert_word_to_hiragana(word)
                else:
                    reading = convert_word_to_hiragana(reading)

                definition_list = []

                for definition in definitions_in_data:
                    definition_text = get_text_only_from_dictionary(word, reading, definition, dictionary_path)
                    if definition_text:
                        definition_list.append(definition_text)
                
                # Create new entry/extend existing one

                if not definition_list:  # No definitions for entry?
                    
                    words_to_remove.append(word)
                    
                else:
                    edit_big_data(big_data, dictionary_path, word, reading, definition_list)


    except JSONDecodeError as e:
        print(f"Error decoding JSON in file {file_path}: {e}")
        return

    # Filter out words to remove
    data_after_edits = [item for item in data if item[0] not in words_to_remove]

    print(f"Size after removals: {len(data_after_edits)}")

    # with open(file_path, "w", encoding="utf-8") as f:
    #     json.dump(data_after_edits, f, ensure_ascii=False, indent=2)

def normalize_references(text: str, dictionary_path: str) -> str:
    text = re.sub(fr" ?[{ARROWS}]", "â‡’", text)
    text = text.replace("\\n", "\n")
    text = text.replace("\\n", "\n")
    flag = False
    text_original = text[:]

    if dictionary_path.endswith("å¤§è¾æ³‰"):

        #ï¼»åï¼½(ã‚¹ãƒ«)ã€Œã‚¢ãƒ«ãƒã‚¤ãƒˆã€ã®ç•¥ã€‚ã€Œå¤ä¼‘ã¿ã«ãƒã‚¤ãƒˆã™ã‚‹ã€
        has_ryaku = re.compile(fr"({PREFIX})ã€Œ([^{OPENING_BRACKETS}]+?)ã€ã®ç•¥({SUFFIX})")
        if has_ryaku.search(text):
            adding_text = has_ryaku.sub(r"â‡’\2", has_ryaku.search(text).group())
            if adding_text not in text:
                text += "\n" + adding_text

        # Handle this?
        # ï¼»é€£èªï¼½â‡’ç½® (ãŠ) ãâ‘«
        # â‡’äººè¿” (ã²ã¨ãŒãˆ) ã—â‘¡

        """
        0-11  â‡’ç•°åŒ– (ã„ã‹) â‘¡
        0-1  
        2-10  ç•°åŒ– (ã„ã‹) 
        2-4   ç•°åŒ–
        10-11 â‘¡
        """

        ref_with_furigana = re.compile(fr"([â‘ -â‘³â¶-â¿ã‰‘-ã‰Ÿâ‘´-â’‡â’ˆ-â’›âŠ-â“â€-â‰ğŸˆ©ğŸˆ”ğŸˆªãŠ€-ãŠ‰ãŠ¤ãŠ¥ãŠ¦ã‹-ã‹¾ï¼‘-ï¼™â“-â“©â’¶-â“ğŸ…-ğŸ…©]|\dï¸âƒ£|^|ã€‚|<br\/>&nbsp;|\n|[ï¼‰ã€\]ã€‘ã€•\)ã€ï¼½ã€‰ã€‹ã€•ã€™ï¼ˆã€Œ\[ã€ã€”\(ã€ï¼»ã€ˆã€Šã€”ã€˜]| |ã€€|è¨˜å·.+?)?â‡’((?:([\u3400-\u4dbf\u4e00-\u9fff\uf900-\ufaff\uff66-\uff9f]+)(?: \([ã‚-ã‚”a-zA-Z]+\) ?)?|[ã‚-ã‚”a-zA-Z]+)+)((?:[â‘ -â‘³â¶-â¿ã‰‘-ã‰Ÿâ‘´-â’‡â’ˆ-â’›âŠ-â“â€-â‰ğŸˆ©ğŸˆ”ğŸˆªãŠ€-ãŠ‰ãŠ¤ãŠ¥ãŠ¦ã‹-ã‹¾ï¼‘-ï¼™â“-â“©â’¶-â“ğŸ…-ğŸ…©]|\dï¸âƒ£)+)?", flags=re.UNICODE)
        match_object = ref_with_furigana.finditer(text)
        match_object_2 = ref_with_furigana.finditer(text)
        links = []
        if match_object:
            for match in match_object:

                # If I ever want to make it actually link stuff up and consider the reading, 
                # I'll do it. But for now, I'll make it ignore the reading.
                # reading = re.findall(fr" \(([{HIRAGANA}a-zA-Z]+)\) ", ref_with_furigana.group()).group()

                no_furigana_and_ref = re.findall(fr"[a-zA-Z]|[\u3400-\u4dbf\u4e00-\u9fff\uf900-\ufaff\uff66-\uff9f]|(?:[â‘ -â‘³â¶-â¿ã‰‘-ã‰Ÿâ‘´-â’‡â’ˆ-â’›âŠ-â“â€-â‰ğŸˆ©ğŸˆ”ğŸˆªãŠ€-ãŠ‰ãŠ¤ãŠ¥ãŠ¦ã‹-ã‹¾ï¼‘-ï¼™â“-â“©â’¶-â“ğŸ…-ğŸ…©]|\dï¸âƒ£)+$|[^(â‡’][ã‚-ã‚”]+[^)]", match.group(), flags=re.U)

                number = match.groups()[3] if match.groups()[3] else ""

                if no_furigana_and_ref:
                    no_furigana_and_ref = "".join([x.replace(" ", "") for x in no_furigana_and_ref])
                
                if no_furigana_and_ref and f"â‡’{no_furigana_and_ref}" not in text:
                    parsed = "Parsed link: " if match.group() != no_furigana_and_ref else ""
                    link = f"{parsed}â‡’{no_furigana_and_ref}"
                    if link not in links:
                        text = fr"{text}\n{link}"  
                        links.append(link)

                if no_furigana_and_ref and no_furigana_and_ref != f"{match.group(2)}{match.group(4) if match.group(4) else ''}":
                    text = text.replace(f"{match.groups()[1:]}", "[linked later]")
                # flag = True

        text = re.sub(r"â‡’{2,}", "â‡’", text)
        text = text.replace("\\n", "\n")
        # convert ã«åŒã˜ format to â‡’ format for linking purposes later.
        # Either in the beginning, between lines, or between periods.
        #                             Prefix   Word        Suffix
        definition_text = re.sub(fr"({PREFIX})ã€Œ(.+?)ã€ã«åŒã˜({SUFFIX})", r"\1â‡’\2\3", text)

        # ã€‚ã€Œè¨€è‘‰â‘ ã€ã«åŒã˜ã€‚
        # ã€‚â‡’è¨€è‘‰â‘ 
        # ã€‚â‡’è¨€è‘‰(1) (Later)


        # ã€‚â‡’å†…åŒ å¯® (ãŸãã¿ã‚Šã‚‡ã†) â‘ 
        # ã€‚â‡’IOAï¼ˆIndependent Olympic Athletesï¼‰
        # â‡’ã‚³ãƒãƒ¼ã‚·ãƒ£ãƒ«â‘ 
        # ...ãƒ©ãƒãƒ¼ã€‚â†’å¼¾æ€§ã‚´ãƒ \nâ‘¡ æ¤ç‰©ã‹ã‚‰...
        # â‡’é‰±å·¥æ¥­ç”Ÿç”£æŒ‡æ•°â‘ 
        #                         To not include the number chars we want later. However, DO include digits, but not if follwoed by emoji data.
        #                       Prefix                         Word                                   SOMETHING IN BRACKETS                        Reference Num or whatever

        pattern_text = fr"([â‘ -â‘³â¶-â¿ã‰‘-ã‰Ÿâ‘´-â’‡â’ˆ-â’›âŠ-â“â€-â‰ğŸˆ©ğŸˆ”ğŸˆªãŠ€-ãŠ‰ãŠ¤ãŠ¥ãŠ¦ã‹-ã‹¾ï¼‘-ï¼™â“-â“©â’¶-â“ğŸ…-ğŸ…©]|\dï¸âƒ£|^|ã€‚|<br\/>&nbsp;|\n|[ï¼‰ã€\]ã€‘ã€•\)ã€ï¼½ã€‰ã€‹ã€•ã€™ï¼ˆã€Œ\[ã€ã€”\(ã€ï¼»ã€ˆã€Šã€”ã€˜]| |ã€€|è¨˜å·.+?)?â‡’([^\n]+?)(?:ï¼ˆ.+?ï¼‰)?((?:[â‘ -â‘³â¶-â¿ã‰‘-ã‰Ÿâ‘´-â’‡â’ˆ-â’›âŠ-â“â€-â‰ğŸˆ©ğŸˆ”ğŸˆªãŠ€-ãŠ‰ãŠ¤ãŠ¥ãŠ¦ã‹-ã‹¾ï¼‘-ï¼™â“-â“©â’¶-â“ğŸ…-ğŸ…©]|\dï¸âƒ£)*?)(ã€‚|\n|<br\/>&nbsp;|$|ãƒ»| |ã€€)"
        pattern = re.compile(pattern_text)
        

        results = pattern.finditer(text)

        if results:
            # if len([x for x in results]) > 1:
                
            for result in results:
                _prefix, word, reference_number, suffix = result.groups()
                reference_number = reference_number if reference_number else ""
                if re.search(fr"[{NUMBER_CHARS}]|\dï¸âƒ£", reference_number):
                    # Reference number
                    reference_numbers = convert_to_path(reference_number) 
                    try:
                        references = "".join([convert_reference_numbers(x) for x in reference_numbers])
                    except KeyError:
                        print("[ERROR]\t", text, reference_numbers)
                else:
                    # Just a suffix
                    references = reference_number

                text = pattern.sub(f"{_prefix}â‡’{word}{references}{suffix}", text)

    if dictionary_path.endswith("ä¸‰çœå ‚å›½èªè¾å…¸"):
        # â‘ æœã€‚åˆå‰ã€‚
        # â‘¡ã€˜æœã€™â†ãƒ¢ãƒ¼ãƒ‹ãƒ³ã‚°ã‚³ãƒ¼ãƒˆã€‚ 
        # â‘¢â†ãƒ¢ãƒ¼ãƒ‹ãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹ã€‚
        # (!) Remeber,   definition = re.sub(fr"[â‡’â‡¨â†â˜]", "â‡’", definition)





        pattern = re.compile(fr"([{NUMBER_CHARS}]|\dï¸âƒ£|^|ã€‚|<br\/>&nbsp;|\n|[{CLOSING_BRACKETS}{OPENING_BRACKETS}]| |ã€€|è¨˜å·.+?)â‡’([{NUMBER_CHARS}]*)([^\d{OPENING_BRACKETS}]+?)([{NUMBER_CHARS}]|\dï¸)?(ãƒ»|$|ã€‚|<br\/>&nbsp;|\n)")
        result = pattern.search(text)
        if result:
            _prefix, _, word, reference_number, suffix = result.groups()
            reference_number = reference_number if reference_number else ""
            if re.search(fr"[{NUMBER_CHARS}]|\dï¸âƒ£", reference_number):
                # Reference number
                reference_numbers = convert_to_path(reference_number) 
                try:
                    references = "".join([convert_reference_numbers(x) for x in reference_numbers])
                except KeyError:
                    print("[ERROR]\t", text, reference_numbers)
            else:
                references = reference_number

            text = pattern.sub(f"{_prefix}â‡’{word}{references}{suffix}", text)

    if dictionary_path.endswith("å¤§è¾æ—"):
        ...


    text = re.sub(rf"ãƒ»(?:{NUMBER_CHARS}]|\dï¸âƒ£|)", "", text)
    return text


def clean_definition(word: str, reading: str, definition_text: str, dictionary_path: str) -> str:
    """
    Cleans and formats the definition text based on the specific dictionary.
    
    Args:
    - definition_text (str): The raw definition text to clean.
    - dictionary_path (str): The name or identifier of the dictionary.

    Returns:
    - str: The cleaned and formatted definition text.
    """
    # Remove the first line for specific dictionaries

    if word.endswith("ã®è§£èª¬"):
        return None

    # Normalize \n's
    definition_text = definition_text.replace("\\n", "\n")
    # Weird character
    definition_text = definition_text.replace("â€‰", " ")

    # Unecessary parts
    definition_text = re.sub(r"(?:\[è£œèª¬\]|ï¼»è£œèª¬ï¼½|ï¼»ç”¨æ³•ï¼½|\[ç”¨æ³•\]|\[å¯èƒ½\]|ï¼»å¯èƒ½ï¼½)(?:.|\n)+", "", definition_text).strip()
    
    # I don't even know why this appears at times
    definition_text = definition_text.replace("_x000D_", "")
    
    # I don't even know why this appears at times
    definition_text = definition_text.replace("_x000D_", "")

    if not word and not reading:
        return None

    # Normalize spaces after numbers:
    definition_text = re.sub(fr"([{NUMBER_CHARS}])[ ]+", r"\1", definition_text)

    definition_text = normalize_references(definition_text, dictionary_path)

    # Using endswith because I don't care about their order in the priority (or what order you chose to give them
    # in the folder name). Just matters that it     ends with the dictionary name.
    if dictionary_path.endswith("å¤§è¾æ³‰"):
        splitted = definition_text.split("\n")
        if len(splitted) > 1:
            definition_text = "<br/>&nbsp;".join(splitted[1:])  # Remove first line
        # Since we're adding all entries as seperate items in a list, we don't need to clean stuff like this;
        # It can easily be filtered out later.
        # No need to destroy data.

        if "[å¯èƒ½]" in definition_text:
            definition_text = definition_text.split("[å¯èƒ½]")[1]

        # Remove remains of example sentences
        # â‘£: ç´å¾—ã™ã‚‹ã€‚åˆç‚¹ãŒã„ãã€‚ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ» (after parsing)
        definition_text = re.sub(r"ãƒ»{2,}", "", definition_text)


        # definition_text = re.sub(r"ï¼»(?:äººåç”¨æ¼¢å­—|å¸¸ç”¨æ¼¢å­—)ï¼½\s*ï¼»éŸ³ï¼½[ã‚¢-ãƒ³]+ï¼ˆæ¼¢ï¼‰\s*[ã‚¢-ãƒ³]+ï¼ˆå‘‰ï¼‰\s*ï¼»è¨“ï¼½[{HIRAGANA}]+(?:{SUFFIX})", "", definition_text)

        # ï¼»å‹•ã‚¶ä¸Šä¸€ï¼½ã€Œã¾ã‚“ï¼ˆæ…¢ï¼‰ãšã‚‹ã€ï¼ˆã‚µå¤‰ï¼‰ã®ä¸Šä¸€æ®µåŒ–ã€‚
        # ï¼»å‹•ã‚¶ä¸Šä¸€ï¼½ã€Œã¿ãã‚“ãšã‚‹ã€ï¼ˆã‚µå¤‰ï¼‰ã®ä¸Šä¸€æ®µåŒ–ã€‚ã€Œè©±é¡Œã®å±•è¦§ä¼šã‚’â€•ãƒ»ã˜ã‚‹ã€
        # ï¼»å‹•ã‚¶ä¸Šä¸€ï¼½ã€Œã¦ã‚“ï¼ˆè»¢ï¼‰ãšã‚‹ã€ï¼ˆã‚µå¤‰ï¼‰ã®ä¸Šä¸€æ®µåŒ–ã€‚ã€Œæ”»å‹¢ã«â€•ãƒ»ã˜ã‚‹ã€

        # First fix ã€Œã¦ã‚“ï¼ˆè»¢ï¼‰ãšã‚‹ã€ â†’ "ã€Œè»¢ãšã‚‹ã€"
        definition_text = re.sub(r"ã€Œ[ã‚-ã‚“]+ï¼ˆ(.+?)ï¼‰(.+?)ã€", r"ã€Œ\1\2ã€", definition_text)

        # Then fix ï¼»å‹•ã‚¶ä¸Šä¸€ï¼½ã€Œè»¢ãšã‚‹ã€ï¼ˆã‚µå¤‰ï¼‰ã®ä¸Šä¸€æ®µåŒ–ã€‚ã€Œæ”»å‹¢ã«â€•ãƒ»ã˜ã‚‹ã€ â†’  "â‡’è»¢ãšã‚‹"
        definition_text = re.sub(fr"(?:ï¼».+?ï¼½)ã€Œ(.+?)ã€ã®(?:..?æ®µåŒ–|..èª)({SUFFIX})", r"â‡’\1\2", definition_text)



        # Remove
        #ï¼»é€£èªï¼½ã€Šå½¢å®¹è©ã€ãŠã‚ˆã³å½¢å®¹è©å‹æ´»ç”¨èªã®é€£ä½“å½¢æ´»ç”¨èªå°¾ã€Œã‹ã‚‹ã€ã«æ¨é‡ã®åŠ©å‹•è©ã€Œã‚ã‚Šã€ã®ä»˜ã„ãŸã€Œã‹ã‚‹ã‚ã‚Šã€ã®éŸ³å¤‰åŒ–ã€‹
        #ï¼»é€£èªï¼½ã€Šé€£èªã€Œã‹ã‚“ã‚ã‚Šã€ã®æ’¥éŸ³ã®ç„¡è¡¨è¨˜ã€‹
        definition_text = re.sub(r"ï¼».+?ï¼½ã€Š.+?ã€‹", r"", definition_text)

        # Remove
        # ã€Œæ†ã’ã«ãŠã—ç«‹ã¡ãŸã‚‹ã“ã¨ãªã©ã¯ã‚ã‚‹ã¾ã˜â€•â—¦ã‚ã‚Šã¨æ€ã™ã‚‚ã®ã‹ã‚‰ã€ã€ˆæºãƒ»è‹¥èœä¸Šã€‰$
        # ã€Œã†ãã¶ã‹ã›çµ¦ãµã“ã¨ã€ã—ã’â€•â—¦ã‚ã‚Šã—ã‹ã°ã€ã€ˆã‹ã’ã‚ãµãƒ»ä¸‹ã€‰$
        definition_text = re.sub(r"^ã€‰.+?ã€ˆã€.+?ã€Œ", r"", definition_text[::-1])[::-1]


        # ï¼»å‹•ãƒ©ä¸‹ä¸€ï¼½ï¼»æ–‡ï¼½ã‹ãã¿ã ãƒ»ã‚‹ï¼»ãƒ©ä¸‹äºŒï¼½
        # ï¼»å‹•ãƒ©äº”ï¼ˆå››ï¼‰ï¼½
        # ï¼»å‹•ã‚µä¸‹ä¸€ï¼½ï¼»æ–‡ï¼½ã‹ãã‚ˆãƒ»ã™ï¼»ã‚µä¸‹äºŒï¼½
        # ï¼»åï¼½(ã‚¹ãƒ«)
        definition_text = re.sub(fr"(?:ï¼».+?ï¼½)+(?:[{HIRAGANA}ãƒ»]+ï¼».+?ï¼½)?(?:\(ã‚¹ãƒ«\))?", r"", definition_text)
    # if my_word:       
    if dictionary_path.endswith("æ—ºæ–‡ç¤¾å›½èªè¾å…¸ ç¬¬åä¸€ç‰ˆ"):
        # Remove first line
        # ã‚ã„â€ã—ã‚‡ã†ã€å“€å‚·ã€‘â€•â€•ã‚·ãƒ¤ã‚¦\n
        splitted = definition_text.split("\n")
        if len(splitted) > 1:
            definition_text = "<br/>&nbsp;".join(splitted[1:])  # Remove first line

        # Remove the first line in items like this.
        # ã‚ã„ã€æŒ¨ã€‘\nã‚¢ã‚¤ãŠ¥\nãŠã™\nç­†é †ï¼š\n
        # \n\nï¼ˆå­—ç¾©ï¼‰\nâ‘  ãŠã™ã€‚æŠ¼ã—ã®ã‘ã‚‹ã€‚ã€ŒæŒ¨æ‹¶ï¼ˆã‚ã„ã•ã¤ï¼‰ï¼ˆï¼åŸç¾©ã¯æŠ¼ã—ã®ã‘ã¦é€²ã‚€æ„ã€‚å›½ ...

        if "ç­†é †ï¼š\n" in definition_text:
            definition_text = definition_text.split("ç­†é †ï¼š\n")[1]
        if "å›³ç‰ˆï¼š" in definition_text:
            definition_text = definition_text.split("å›³ç‰ˆï¼š")[1]

        # Remove 
        # ï¼ˆåãƒ»ä»–ã‚¹ãƒ«ï¼‰\n.
        # ï¼ˆå½¢ï¼‰ã€Šã‚«ãƒ­ãƒ»ã‚«ãƒ„ï¼ˆã‚¯ï¼‰ãƒ»ã‚¤ãƒ»ã‚¤ãƒ»ã‚±ãƒ¬ãƒ»â—‹ã€‹\n
        # But keep (â€¦ã®ç•¥).

        definition_text = re.sub(r"ï¼ˆ.+?ï¼‰(ã€Š.+?ã€‹)?\n", "", definition_text)

        # Remove 
        # ã€”å¯èƒ½ã€•ã‚ãŒãƒ»ã‚Œã‚‹ï¼ˆä¸‹ä¸€ï¼‰<br/>&nbsp;
        # ã€”ä»–ã€•ã‚ãƒ»ã’ã‚‹ï¼ˆä¸‹ä¸€ ï¼‰
        definition_text = re.sub(r"ã€”.+?ã€•?[{HIRAGANA}ãƒ»]+ï¼ˆ.+?ï¼‰({SUFFIX})", r"\1", definition_text)

        # Remove everything after ã€˜ä½¿ã„åˆ†ã‘ã€™
        if "ã€˜ä½¿ã„åˆ†ã‘ã€™" in definition_text:
            definition_text = definition_text.split("ã€˜ä½¿ã„åˆ†ã‘ã€™")[0]

        # Remove everything after ã€˜ã¡ãŒã„ã€™
        if "ã€˜ã¡ãŒã„ã€™" in definition_text:
            definition_text = definition_text.split("ã€˜ã¡ãŒã„ã€™")[0]

    if dictionary_path.endswith("ä½¿ã„æ–¹ã®åˆ†ã‹ã‚‹ é¡èªä¾‹è§£è¾å…¸"):
        ...
        # This is already handled in the scraping function.
        # definition_text = definition_text.split(r'ğŸ“šä½¿ã„æ–¹')[0]
        # definition_text = definition_text.split(r'ğŸ”„ä½¿ã„åˆ†ã‘')[0]
        # definition_text = definition_text.split(r'ğŸ”—é–¢é€£èª')[0]
    
    if dictionary_path.endswith("ä¸‰çœå ‚å›½èªè¾å…¸"):
        ...
        # This is already handled in the scraping function
        # definition_text = re.sub(r"^.+?ï½ <br/>&nbsp;|ã€Œ.+ã€(?:<br/>&nbsp;)?", "", definition_text)
    
    if dictionary_path.endswith("äº‹æ•…ãƒ»ã“ã¨ã‚ã–ãƒ»æ…£ç”¨å¥ã‚ªãƒ³ãƒ©ã‚¤ãƒ³"):
        ...
        # This is already handled in the scraping function

        # Remove spans like this
        # ã—ã‚Šã¦ã—ã‚‰ã–ã‚Œã€çŸ¥ã‚Šã¦çŸ¥ã‚‰ã–ã‚Œã€‘
        # ã€å¤±æ•—ã¯æˆåŠŸã®ã‚‚ã¨ã€‘

    if dictionary_path.endswith("å¤§è¾æ—"):
        no_period_quote = re.search(r"[^ã€‚ã€]$", definition_text)
        final_word_reference = re.search(fr"â‡’[{KANJI}{HIRAGANA}a-zA-Z]+$", definition_text)
        if no_period_quote and not final_word_reference:
            return None
        definition_text = definition_text.split("è£œèª¬æ¬„")[0]

        # This is already handled in the scraping function
    if dictionary_path.endswith("å®Ÿç”¨æ—¥æœ¬èªè¡¨ç¾è¾å…¸"):
        ...
        # This is already handled in the scraping function

    if dictionary_path.endswith("Weblio"):
        #ï¼»å‹•ã‚«ä¸‹ä¸€ï¼½ï¼»æ–‡ï¼½ãªã¤ãƒ»ãï¼»ã‚«ä¸‹äºŒï¼½ã€Šã€Œãªã¥ã‘ã‚‹ã€ã¨ã‚‚ã€‹
        #ï¼»å‹•ã‚¢ä¸‹ä¸€ï¼½ï¼»æ–‡ï¼½ã‹ã¾ãƒ»ãµï¼»ãƒä¸‹äºŒï¼½
        #ï¼»å‹•ã‚«äº”ï¼ˆå››ï¼‰ï¼½
        definition_text = re.sub(fr"({PREFIX})(?:ï¼».+?ï¼½)+(?:[{HIRAGANA}ãƒ»]+ï¼».+?ï¼½)?(?:ã€Š.+?ã€‹)?({SUFFIX})", r"\1\2", definition_text)

    # # Add line breaks before entry numbers
    # definition_text = re.sub(fr"([{NUMBER_CHARS}]|\dï¸âƒ£)", r"<br/>&nbsp;\1", definition_text)
    # Clean up leading or trailing unwanted characters

    if definition_text:
        definition_text = definition_text.strip("\n").strip("<br/>&nbsp;")
        # once

    # if "â‡’" in definition_text:
    #     definition_text = re.sub(fr"({PREFIX})â‡’([{NUMBER_CHARS}]*)(.+)($|ã€‚|<br/>&nbsp;|\n)", r"\1\2\3\4", definition_text)

    # Normalize numbers back
    definition_text = re.sub(fr"([{NUMBER_CHARS}][^ ]) ", r"\1 ", definition_text)

    # Normalize line breaks
    definition_text = definition_text.replace("\n", "<br/>&nbsp;").replace("\\n", "<br/>&nbsp;")

    # Contract multiple linebreaks into a single linebreak
    definition_text = re.sub(fr"(?:<br/>&nbsp;){2,}", r"<br/>&nbsp;", definition_text)
    # Temp
    definition_text = definition_text.replace("<br/>&nbsp;", "\n")

    definition_dict = recursive_nesting_by_category(definition_text)
    if isinstance(definition_dict, dict):
        definition_text = dict_to_text(definition_dict)
    else:
        definition_text = definition_dict

    definition_text = re.sub(r"ã€‚{2,}", "", definition_text)
    
    

    definition_text = definition_text.strip("\n").strip()
    return definition_text   


def get_text_only_from_dictionary(word: str, reading: str, definition_data: list, dic_name: str) -> str:
    """
    Extracts the main definition text from the raw data.

    Args:
    - definition_data (list): List of strings and possibly other data types containing the definition.
    - dic_name (str): Name of the dictionary being processed.

    Returns:
    - str: Cleaned and simplified definition text.
    """


    def get_non_recursive(information):
        """Iteratively extracts strings from the nested structure."""
        stack = [information]
        result = []

        while stack:
            current = stack.pop()
            if isinstance(current, str):
                """
                Add in a seperator, so later we can filter out 
                unwanted text easily.
                Make sure the seperator definitely won't appear 
                in the definitions
                """
                result.append(current)   

            elif isinstance(current, list):
                flag = True
                """
                ä½¿ã„æ–¹ã®åˆ†ã‹ã‚‹ é¡èªä¾‹è§£è¾å…¸
                "content": [
                  {
                    "tag": "span",
                    "style": {
                      "fontWeight": "bold"
                    },
                    "content": "ãã†"
                  },
                  {
                    "tag": "span",
                    "style": {
                      "fontWeight": "normal"
                    },
                    "content": "ã€åƒ§ã€‘åƒ§ï¼åƒ§ä¾¶ï¼åŠä¸»ï¼åŠã•ã‚“ï¼å¾¡åŠï¼ãŠå¯ºã•ã‚“ï¼åƒ§å®¶ï¼æ²™é–€ï¼æ³•å¸«ï¼å‡ºå®¶ï¼æ¯”ä¸˜"
                  }
                ]
                ---------------------------------
                å®Ÿç”¨æ—¥æœ¬èªè¡¨ç¾è¾å…¸
                "content": [
                  {
                    "tag": "span",
                    "style": {
                      "fontWeight": "bold"
                    },
                    "content": "ã”ã˜ã‚ã„"
                  },
                  {
                    "tag": "span",
                    "style": {
                      "fontWeight": "normal"
                    },
                    "content": "ã€ã”è‡ªæ„›ã€‘"
                  }
                ]
                """

                if len(current) == 2:
                    first, second = current
                    # if dic_name.endswith("ä½¿ã„æ–¹ã®åˆ†ã‹ã‚‹ é¡èªä¾‹è§£è¾å…¸"):
                    #     reading_data, ruigigo_data = first, second

                    #     if "content" in ruigigo_data and "content" in reading:
                    #         if isinstance(ruigigo_data["content"], str):
                    #             actually_is_ruigigo = re.search(r"(.+?ï¼)+(.+)", ruigigo_data["content"])
                    #             if actually_is_ruigigo:
                    #                 # The actual é¡ç¾©èª part.
                    #                 flag = False

                    # These two are essentially the same thing.
                    if dic_name.endswith("å®Ÿç”¨æ—¥æœ¬èªè¡¨ç¾è¾å…¸") or dic_name.endswith("ä½¿ã„æ–¹ã®åˆ†ã‹ã‚‹ é¡èªä¾‹è§£è¾å…¸"):
                        if "content" in first:
                            if first["content"] == reading:
                                flag = False


                if flag:
                    stack.extend(reversed(current))  # Maintain original order

            elif isinstance(current, dict):
                flag = True
                content = current.get("content")
                if "data" in current:
                    if "name" in current["data"]:
                        current_name = current["data"]["name"]
                        # Some of these have a "G" suffix, some don't, etc.
                        unwanted_tags = re.compile("è¡¨è¨˜|å“è©|ç”¨ä¾‹|æ³¨è¨˜|æ­´å²ä»®å|åŒºåˆ¥|ãƒ«ãƒ“|è¦‹å‡º|å¯èƒ½å½¢|ç•°å­—åŒè¨“")

                        if dic_name.endswith("å¤§è¾æ—"):
                            if current_name == "ref":
                                if isinstance(content, dict) and "content" in content:
                                    # We already fetched it. This is current[content][content]
                                    if isinstance(content, str):
                                         if re.search(fr"^[{NUMBER_CHARS}]$", content["content"]):
                                            # Convert references to items in the same entry to something
                                            # that wont be picked up by the regexes later
                                            content["content"] = f'({REFERENCE_NUMBER_MAP[content["content"]]})'
                            
                            elif current_name == "å˜ä½å" and "content" in current["data"]:
                                if isinstance(current["data"]["content"], str):
                                # (ã‚»ãƒ³ãƒãƒ¡ãƒ¼ãƒˆãƒ«)
                                    current["data"]["content"] = current["data"]["content"][1:-1]
                            elif unwanted_tags.search(current_name):
                                flag = False

                        if dic_name.endswith("ä½¿ã„æ–¹ã®åˆ†ã‹ã‚‹ é¡èªä¾‹è§£è¾å…¸") and current_name != "æ„å‘³":
                            flag = False
                    
                        if dic_name.endswith("ä¸‰çœå ‚å›½èªè¾å…¸"):
                            """
                                {
                                  "tag": "span",
                                  "data": {
                                    "name": "å‚ç…§èªç¾©ç•ªå·"
                                  },
                                  "content": {
                                    "tag": "span",
                                    "data": {
                                      "name": "èªç¾©ç•ªå·"
                                    },
                                    "content": "â‘ "
                                  }
                                }
                            """

                            if unwanted_tags.search(current_name):
                                # Pretty sure ä¸‰çœå ‚å›½èªè¾å…¸ doesn't have this but å¤§è¾æ— does.
                                flag = False

                            elif "å‚ç…§èªç¾©ç•ªå·" in current_name:
                                if "content" in content:
                                    if isinstance(content["content"], str):
                                        reference_number = re.search(fr"^({NUMBER_CHARS})$", content["content"])
                                        if reference_number:
                                            content["content"] = f'({REFERENCE_NUMBER_MAP[content["content"]]})'
                    
                        # å®Ÿç”¨æ—¥æœ¬èªè¡¨ç¾è¾å…¸ doesn't seem to have any names other than "definition",
                        # but I put this here just in case.
                        if dic_name.endswith("å®Ÿç”¨æ—¥æœ¬èªè¡¨ç¾è¾å…¸"):
                            if current_name != "definition":
                                flag = False


                if dic_name.endswith("äº‹æ•…ãƒ»ã“ã¨ã‚ã–ãƒ»æ…£ç”¨å¥ã‚ªãƒ³ãƒ©ã‚¤ãƒ³"):
                    if "tag" in current:
                        tag = current["tag"]
                        """
                           Span example:
                          {
                            "tag": "span",
                            "content": "ã—ã®ã—ã‚‡ã†ã«ã‚“ã€æ­»ã®å•†äººã€‘"
                          },
                          Tables are are just the ç•°å½¢s summarized in table form.
                        """
                        if tag in ["span", "table"]:
                            flag = False

                if dic_name.endswith("ä¸‰çœå ‚å›½èªè¾å…¸"):
                    ...

                if flag:
                    if content:
                        stack.append(content)
            else:
                print(f"Unexpected type encountered in dictionary '{dic_name}': {type(current)}")  # Logging unexpected types
        return "".join(result)

    my_text = get_non_recursive(definition_data)
    return clean_definition(word, reading, my_text, dic_name)

def edit_big_data(big_data, dictionary_path, word, reading, definitions):
    # Given a word, its reading, and its definition, it creates a new datapoint 
    # for said word/reading.

    """
    big_data = {
        "dictionary_path": {
            "word": {
                "reading1": ["definitions_1"],
                "reading2": ["definitions_2"],
            }
        }
    }
    """
    
    if word not in big_data[dictionary_path]:
        big_data[dictionary_path][word] = {}

    if reading not in big_data[dictionary_path][word]:
        big_data[dictionary_path][word][reading] = []
    
    if definitions:
        definitions = [x for x in definitions if "Weblio" not in x]
        big_data[dictionary_path][word][reading].extend(definitions)
        # Just in case there's dupelicates
        big_data[dictionary_path][word][reading] = list(set(big_data[dictionary_path][word][reading]))


def load_big_data(big_data_dictionary, override=False):
    if not override:
        with open("big_data.json", "r", encoding="utf-8") as f:
            return json.load(f)
    else:
        print("You're about to override big_data, continue?\ny\\N")
        user_choice = input()
        if user_choice != "y":
            exit()
        for dictionary_path in PRIORITY_ORDER:
            add_dictionary_to_big_data(dictionary_path, big_data_dictionary)

        # add_dictionary_to_big_data("6. æ—ºæ–‡ç¤¾å›½èªè¾å…¸ ç¬¬åä¸€ç‰ˆ", big_data_dictionary)
        # add_dictionary_to_big_data("4. ä½¿ã„æ–¹ã®åˆ†ã‹ã‚‹ é¡èªä¾‹è§£è¾å…¸", big_data_dictionary)
        # add_dictionary_to_big_data("8. Weblio", big_data_dictionary)

        # Write the final big_data to a JSON file
        save_to_big_data(big_data_dictionary)
        return big_data_dictionary

def save_to_big_data(big_data_dictionary):
    with open(BIG_DATA_FILE, "w", encoding="utf-8") as f:
        json.dump(big_data_dictionary, f, ensure_ascii=False, indent=2)
    print("Saved to big data")

if __name__ == "__main__":

    big_data_dictionary = load_big_data(big_data_dictionary, override=True)
    